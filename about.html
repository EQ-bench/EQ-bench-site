<!DOCTYPE html>
<html lang="en">
<head>	
	<meta charset="UTF-8">
	<title>EQ-Bench Leaderboard</title>
	<!-- Bootstrap CSS -->
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<!-- DataTables Bootstrap CSS -->
	<link rel="stylesheet" type="text/css" href="https://cdn.datatables.net/1.10.19/css/dataTables.bootstrap4.min.css">
	<link rel="stylesheet" type="text/css" href="style.css">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="pragma" content="no-cache">
</head>
<body>
	<div class="container mt-4">
		<!-- Dark/Light Mode Toggle -->
		<div class="form-check form-switch">
			<input class="form-check-input" type="checkbox" id="darkModeToggle">
			<label class="form-check-label" for="darkModeToggle" id="toggleLabel">Light</label>
		</div>
		<div class="header">
			<img src="./images/eqbench_logo_sml.png" alt="EQ-bench Logo" class="logo"/>
			<div class="header-text">
				<h1>EQ-Bench</h1>				 
			</div>
		</div>
	  <p>Emotional Intelligence Benchmark for LLMs</p>
		
		<p><a href="https://github.com/EQ-bench/EQ-Bench" target="_blank">Github</a> | <a href="https://arxiv.org/abs/2312.06281" target="_blank">Paper</a> | <span id="email"></span> | <a href="https://twitter.com/sam_paech" target="_blank">Twitter</a> | <a href="index.html">Leaderboard</a></p>

		<h3>About</h3>
		<p>EQ-Bench is a benchmark for language models designed to assess emotional intelligence.</p>
		<p>Why emotional intelligence? One reason is that it represents a subset of abilities that are important for the user experience, and which isn't explicitly tested by other benchmarks. Another reason is that it's not trivial to improve scores by fine tuning for the benchmark, which makes it harder to "game" the leaderboard.</p>
		<p>EQ-Bench is a little different from traditional psychometric tests. It uses a specific question format, in which the subject has to read a dialogue then rate the intensity of possible emotional responses of one of the characters. Every question is interpretative and assesses the ability to predict the magnitude of the 4 presented emotions. The test is graded without the need for a judge (so there is no length bias). It's cheap to run (only 171 questions), and produces results that correlate strongly with human preference (Arena ELO) and multi-domain benchmarks like MMLU.</p>
		<p>You can run the benchmark on your own models or validate the leaderboard scores using the code in the github repo above.</p>
		<p>If you would like to see a model on the leaderboard, get in touch and suggest it!</p>
		<br>
		<p><b>MAGI</b> is a recently added metric to the leaderboard. It is a custom subset of MMLU and AGIEval, selected to have strong discriminatory power amongst the top models. It's still in development, but the test set and selection methodology will be released publicly soon (TM).</p>
		<hr>
		<h5>Cite EQ-Bench:</h5>
		<pre><code>
@misc{paech2023eqbench,
	title={EQ-Bench: An Emotional Intelligence Benchmark for Large Language Models}, 
	author={Samuel J. Paech},
	year={2023},
	eprint={2312.06281},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}
		</code></pre>	
		MAGI draws from the MMLU and AGIEval tests. <span class="clickable-text" id="expando-btn" style="cursor: pointer; text-decoration: underline; color: blue;">Click to show citations</span>
		<div class="expando-content mt-3" style="overflow-x: auto;">
			 <pre><code>
@article{hendryckstest2021,
		title={Measuring Massive Multitask Language Understanding},
		author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
		journal={Proceedings of the International Conference on Learning Representations (ICLR)},
		year={2021}
}

@article{hendrycks2021ethics,
		title={Aligning AI With Shared Human Values},
		author={Dan Hendrycks and Collin Burns and Steven Basart and Andrew Critch and Jerry Li and Dawn Song and Jacob Steinhardt},
		journal={Proceedings of the International Conference on Learning Representations (ICLR)},
		year={2021}
}

@misc{zhong2023agieval,
		title={AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models},
		author={Wanjun Zhong and Ruixiang Cui and Yiduo Guo and Yaobo Liang and Shuai Lu and Yanlin Wang and Amin Saied and Weizhu Chen and Nan Duan},
		year={2023},
		eprint={2304.06364},
		archivePrefix={arXiv},
		primaryClass={cs.CL}
}

@inproceedings{ling-etal-2017-program,
		title = "Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems",
		author = "Ling, Wang  and
		Yogatama, Dani  and
		Dyer, Chris  and
		Blunsom, Phil",
		booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
		month = jul,
		year = "2017",
		address = "Vancouver, Canada",
		publisher = "Association for Computational Linguistics",
		url = "https://aclanthology.org/P17-1015",
		doi = "10.18653/v1/P17-1015",
		pages = "158--167",
}

@inproceedings{hendrycksmath2021,
		title={Measuring Mathematical Problem Solving With the MATH Dataset},
		author={Dan Hendrycks and Collin Burns and Saurav Kadavath and Akul Arora and Steven Basart and Eric Tang and Dawn Song and Jacob Steinhardt},
		journal={NeurIPS},
		year={2021}
}

@inproceedings{Liu2020LogiQAAC,
		title={LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning},
		author={Jian Liu and Leyang Cui and Hanmeng Liu and Dandan Huang and Yile Wang and Yue Zhang},
		booktitle={International Joint Conference on Artificial Intelligence},
		year={2020}
}

@inproceedings{zhong2019jec,
		title={JEC-QA: A Legal-Domain Question Answering Dataset},
		author={Zhong, Haoxi and Xiao, Chaojun and Tu, Cunchao and Zhang, Tianyang and Liu, Zhiyuan and Sun, Maosong},
		booktitle={Proceedings of AAAI},
		year={2020},
}

@article{Wang2021FromLT,
		title={From LSAT: The Progress and Challenges of Complex Reasoning},
		author={Siyuan Wang and Zhongkun Liu and Wanjun Zhong and Ming Zhou and Zhongyu Wei and Zhumin Chen and Nan Duan},
		journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
		year={2021},
		volume={30},
		pages={2201-2216}
}
			 </code></pre>
		</div>

		<br>
		<hr>
	</div>

	<!-- jQuery and Bootstrap JS -->
	<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"></script>
	<script src="https://cdn.datatables.net/1.10.19/js/jquery.dataTables.min.js"></script>
	<script src="https://cdn.datatables.net/1.10.19/js/dataTables.bootstrap4.min.js"></script>
	<script src="script.js"></script>
</body>
</html>