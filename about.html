<!DOCTYPE html>
<html lang="en">
<head>	
	<meta charset="UTF-8">
	<title>EQ-Bench Leaderboard</title>
	<!-- Bootstrap CSS -->
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<!-- DataTables Bootstrap CSS -->		
	<link rel="stylesheet" type="text/css" href="https://cdn.datatables.net/1.10.19/css/dataTables.bootstrap4.min.css">
	<link rel="stylesheet" type="text/css" href="style.css">
	<link rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.4/css/lightbox.min.css">
	<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.7.1/jquery.min.js"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.4/js/lightbox-plus-jquery.min.js"></script>

	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="pragma" content="no-cache">
</head>
<body>
	<div class="container mt-4">
		<!-- Dark/Light Mode Toggle -->
		<div class="form-check form-switch">
			<input class="form-check-input" type="checkbox" id="darkModeToggle">
			<label class="form-check-label" for="darkModeToggle" id="toggleLabel">Light</label>
		</div>
		<div class="header">
			<a href="./"><img src="./images/eqbench_logo_sml.png" alt="EQ-bench Logo" class="logo"/></a>
			<div class="header-text">
				<h1>EQ-Bench</h1>				 
			</div>
		</div>
	  <p>Emotional Intelligence Benchmark for LLMs</p>
		
		<p><a href="https://github.com/EQ-bench" target="_blank">Github</a> | <a href="https://arxiv.org/abs/2312.06281" target="_blank">Paper</a> | <span id="email"></span> | <a href="https://twitter.com/sam_paech" target="_blank">Twitter</a> | <a href="index.html">Leaderboard</a></p>

		<p class="nav-links">
			<a href='index.html'>üíôEQ-Bench3</a>
			<span class="nav-separator">|</span>
			<a href='creative_writing_longform.html'>‚úçÔ∏èLongform Writing</a>
			<span class="nav-separator">|</span>
			<a href='creative_writing.html'>üé®Creative Writing v3</a>
			<span class="nav-separator">|</span>
			<a href='judgemark-v2.html'>‚öñÔ∏èJudgemark v2.1</a>
			<span class="nav-separator">|</span>
			<a href='buzzbench.html'>üé§BuzzBench</a>
			<span class="nav-separator">|</span>
			<a href='diplobench.html'>üåçDiploBench</a>
			<span class="nav-separator">|</span>
			<a href='creative_writing_v2.html'>üé®Creative Writing (Legacy)</a>
			<span class="nav-separator">|</span>
			<a href='eqbench-v2.html'>üíóEQ-Bench (Legacy)</a>
		</p>
		

		<p><script type='text/javascript' src='https://storage.ko-fi.com/cdn/widget/Widget_2.js'></script><script type='text/javascript'>kofiwidget2.init('Support Me on Ko-fi', '#1a1a1a', 'O5O7VUVYO');kofiwidget2.draw();</script> </p>

		<div class="toc">
			<ul>
				 <li><a href="#about">How to Submit</a></li>
				 <li>
					EQ-Bench 3:
					<a href="#short" class="toc-link">Short</a>
					|
					<a href="#long"  class="toc-link">Long</a>
				  </li>
				  
				 <li><a href="#eq-bench">EQ-Bench</a></li>
				 <li><a href="#magi-hard">MAGI-Hard</a></li>
				 <li><a href="#creative-writing">Creative Writing</a></li>
				 <li><a href="#judgemark">Judgemark</a></li>
				 <li><a href="#buzzbench">BuzzBench</a></li>
				 <li><a href="#diplobench">DiploBench</a></li>
				 <li><a href="#citations">Citations</a></li>
			</ul>
	  </div>
		
		<div id="about" class="section">
			<h3>üì©How to Submit</h3>
			<p>At this time we only accept submissions of open weight models that are available to everyone via HuggingFace.</p>
			<p>To submit, get in touch by email or twitter with:
				<ul>
					<li>A link to your model on huggingface</li>
					<li>Optimal prompt format & generation config</li>
					<li>The EQ-Bench score that you got for your model</li>
				</ul>
			</p>
			<p>We will then verify the result on our end and add to the leaderboard. This project is self funded so please respect that we don't have unlimited compute!</p>
		</div>
		




		<style>
			/* Define theme colors (adjust as needed) */
			:root {
				--bs-primary: #0d6efd;
				--bs-info: #0dcaf0;
				/* Dark mode equivalents */
				--bs-primary-dark: #6ea8fe;
				--bs-info-dark: #6edff6;
				/* Default background/text colors (used in some blocks) */
				--bg-color: #f8f9fa;
				--border-color: #dee2e6; /* Adjusted from judging prompt's #6c757d for broader use */
				--text-color: #212529;
				--heading-color: #343a40;
				/* Subtle variations for nested blocks */
				--bg-color-subtle: #f8f9fa;
				--border-color-subtle: #dee2e6;
			}
		
			/* Dark Mode Root Variables */
			.dark-mode, [data-theme="dark"] {
				--bg-color: #2d3238;
				--border-color: #495057; /* Adjusted from judging prompt's #90a4ae */
				--text-color: #e9ecef;
				--heading-color: #f8f9fa;
				--bg-color-subtle: #3a4148;
				--border-color-subtle: #495057;
			}
		
			/* Accordion Styling */
			.accordion-container {
				margin-bottom: 1.5rem; /* Add some space below the accordion */
			}
		
			.accordion-header {
				cursor: pointer;
				padding: 0.75rem 1.25rem;
				background-color: var(--accordion-header-bg, #e9ecef); /* Use CSS variables for theme compatibility */
				border: 1px solid var(--accordion-border-color, #dee2e6);
				margin-top: 0.5rem;
				margin-bottom: 0; /* Remove bottom margin to connect with content */
				font-weight: bold;
				position: relative; /* Needed for absolute positioning of indicator */
				border-radius: 0.25rem 0.25rem 0 0; /* Round top corners */
				transition: background-color 0.2s ease-in-out;
			}
		
			.accordion-header:hover {
				background-color: var(--accordion-header-hover-bg, #d1d9e0);
			}
		
			.accordion-content {
				padding: 1rem 1.25rem;
				border: 1px solid var(--accordion-border-color, #dee2e6);
				border-top: none; /* Remove top border as header has bottom border */
				display: none; /* Hidden by default */
				background-color: var(--accordion-content-bg, #ffffff); /* Content background */
				border-radius: 0 0 0.25rem 0.25rem; /* Round bottom corners */
				margin-bottom: 0.5rem; /* Space below content when open */
			}
		
			.accordion-header.active {
				background-color: var(--accordion-header-active-bg, #d1d9e0);
				border-bottom-left-radius: 0; /* Flatten bottom corners when active */
				border-bottom-right-radius: 0;
			}
		
			.accordion-content.active {
				display: block; /* Show when active */
			}
		
			.accordion-indicator {
				position: absolute;
				right: 1.25rem;
				top: 50%;
				transform: translateY(-50%);
				font-size: 1.2em;
				font-weight: bold;
				transition: transform 0.2s ease-in-out;
			}
		
			/* Indicator symbols */
			.accordion-header:not(.active) .accordion-indicator::before {
				content: '+';
			}
		
			.accordion-header.active .accordion-indicator::before {
				content: '‚àí'; /* Minus sign */
			}
		
		
			/* Dark Mode Accordion Adjustments */
			.dark-mode .accordion-header, [data-theme="dark"] .accordion-header {
				--accordion-header-bg: #343a40;
				--accordion-border-color: #495057;
				color: #e9ecef;
			}
		
			.dark-mode .accordion-header:hover, [data-theme="dark"] .accordion-header:hover {
				--accordion-header-hover-bg: #495057;
			}
		
			.dark-mode .accordion-header.active, [data-theme="dark"] .accordion-header.active {
				--accordion-header-active-bg: #495057;
			}
		
			.dark-mode .accordion-content, [data-theme="dark"] .accordion-content {
				--accordion-content-bg: #2d3238;
				--accordion-border-color: #495057;
				color: #e9ecef;
			}
		
			/* Ensure nested elements inherit dark mode colors if needed */
			.dark-mode .accordion-content p,
			.dark-mode .accordion-content li,
			.dark-mode .accordion-content h4,
			.dark-mode .accordion-content h5, /* Added h5 */
			.dark-mode .accordion-content pre,
			.dark-mode .accordion-content strong, /* Added strong */
			[data-theme="dark"] .accordion-content p,
			[data-theme="dark"] .accordion-content li,
			[data-theme="dark"] .accordion-content h4,
			[data-theme="dark"] .accordion-content h5, /* Added h5 */
			[data-theme="dark"] .accordion-content pre,
			[data-theme="dark"] .accordion-content strong { /* Added strong */
				color: inherit; /* Inherit from .accordion-content */
			}
		
		
			/* Styles specific to EQ-Bench 3 section content */
		
			/* General Content Styling */
			.long-version-content h4 {
				margin-top: 1.5rem; /* Add space above main headings */
				margin-bottom: 0.75rem;
			}
			.long-version-content h4:first-of-type {
				margin-top: 0; /* No extra top margin for the very first heading */
			}
			.analysis-img {
				max-width: 100%;
				height: auto;
				display: block;
				margin: 1rem 0; /* Adjusted margin */
				border: 1px solid var(--border-color-subtle);
				border-radius: 0.25rem;
			}
		
			/* Intro Paragraph Styling */
			.long-version-content .intro-paragraph {
				font-size: 1.05em; /* Slightly larger intro text */
				margin-bottom: 1.5rem;
			}
		
			/* Key Points List Styling */
			.long-version-content .key-points-list {
				list-style: none; /* Remove default bullets */
				padding-left: 0;
				margin-bottom: 1.5rem;
			}
			.long-version-content .key-points-list li {
				padding: 0.5rem 0 0.5rem 1.5rem;
				position: relative;
				margin-bottom: 0.25rem;
				border-left: 3px solid var(--accordion-header-bg, #e9ecef); /* Use theme color */
			}
			.long-version-content .key-points-list li::before {
				content: '‚úì'; /* Checkmark or other symbol */
				position: absolute;
				left: 0.25rem;
				top: 0.5rem; /* Adjust vertical alignment */
				color: var(--bs-primary);
				font-weight: bold;
			}
			/* Dark mode adjustments for key points list */
			.dark-mode .long-version-content .key-points-list li,
			[data-theme="dark"] .long-version-content .key-points-list li {
				border-left-color: var(--accordion-header-active-bg, #495057); /* Use active header bg for contrast */
			}
			.dark-mode .long-version-content .key-points-list li::before,
			[data-theme="dark"] .long-version-content .key-points-list li::before {
				color: var(--bs-primary-dark);
			}
		
			/* Problem Comparison Grid */
			.problem-comparison-container {
				display: grid;
				grid-template-columns: 1fr; /* Default to single column */
				gap: 1rem;
				margin: 1.5rem 0;
			}
			@media (min-width: 768px) { /* Apply two columns on medium screens and up */
				.problem-comparison-container {
					grid-template-columns: 1fr 1fr;
				}
			}
			.problem-category {
				background-color: var(--bg-color-subtle);
				border: 1px solid var(--border-color-subtle);
				border-radius: 0.25rem;
				padding: 1rem;
			}
			.problem-category h5 {
				margin-top: 0;
				margin-bottom: 0.75rem;
				font-weight: 600;
				border-bottom: 1px solid var(--border-color-subtle);
				padding-bottom: 0.5rem;
				color: var(--heading-color); /* Ensure heading color */
			}
			.problem-category ul {
				padding-left: 1.25rem;
				margin-bottom: 0;
			}
			.problem-category ul li {
				margin-bottom: 0.3rem;
			}
			/* Dark mode handled by root variables and inheritance */
		
			/* EQ-Bench Approach Callout */
			.eq-bench-approach {
				background-color: var(--accordion-content-bg, #ffffff); /* Match content background */
				border-left: 4px solid var(--bs-info); /* Use an accent color */
				padding: 1rem 1rem 1rem 1.5rem;
				margin: 1.5rem 0;
				border-radius: 0 0.25rem 0.25rem 0;
			}
			/* Dark mode adjustments for approach callout */
			.dark-mode .eq-bench-approach,
			[data-theme="dark"] .eq-bench-approach {
				background-color: var(--accordion-content-bg, #2d3238);
				border-left-color: var(--bs-info-dark);
			}
		
			/* Authoring Process Block */
			.authoring-process {
				margin-top: 1rem; /* Space before this section */
				padding: 1rem;
				background-color: var(--bg-color-subtle);
				border: 1px solid var(--border-color-subtle);
				border-radius: 0.25rem;
			}
			.authoring-process ol {
				margin-top: 0.75rem;
				padding-left: 1.5rem;
			}
			.authoring-process ol li {
				margin-bottom: 0.5rem;
			}
			/* Dark mode handled by root variables */
		
			/* Task Archetypes List (within Authoring Process) */
			.task-archetypes {
				margin-top: 1rem;
				padding-top: 1rem;
				border-top: 1px dashed var(--border-color-subtle); /* Separator within the block */
			}
			.task-archetypes ul {
				list-style-type: disc;
				padding-left: 1.5rem;
			}
			.task-archetypes ul li {
				margin-bottom: 0.4rem;
			}
			/* Dark mode handled by root variables */
		
			/* Score Meaning Block */
			.score-meaning {
				margin-top: 1.5rem;
				padding: 1rem;
				background-color: var(--bg-color-subtle);
				border: 1px solid var(--border-color-subtle);
				border-radius: 0.25rem;
			}
			.score-meaning p:first-child {
				margin-top: 0;
			}
			.score-meaning strong {
				color: var(--heading-color); /* Make strong text stand out a bit */
			}
			/* Dark mode handled by root variables and inheritance */
		
		
			/* Criteria Block Styling (Original) */
			.criteria-block {
				background-color: var(--bg-color); /* Use root variable */
				border: 1px solid var(--border-color); /* Use root variable */
				border-radius: 0.25rem;
				padding: 1rem;
				margin-bottom: 1.5rem;
				font-family: monospace;
				color: var(--text-color); /* Use root variable */
			}
			.criteria-grid {
				display: grid;
				grid-template-columns: repeat(auto-fill, minmax(200px, 1fr));
				gap: 0.5rem;
			}
			.criteria-item {
				padding: 0.25rem 0;
			}
			.criteria-list {
				margin: 0;
				padding-left: 1.5rem;
			}
			.criteria-list li {
				padding: 0.25rem 0;
			}
			@media (max-width: 576px) {
				.criteria-grid {
					grid-template-columns: repeat(auto-fill, minmax(150px, 1fr));
				}
			}
			/* Dark mode handled by root variables */
			/* Specific dark mode overrides if needed (original had slightly different colors) */
			.dark-mode .criteria-block, [data-theme="dark"] .criteria-block {
				--bg-color: #3a4148; /* Slightly different background for nested blocks */
				--border-color: #5a636b;
				--text-color: #ced4da;
				background-color: var(--bg-color); /* Apply local override */
				border-color: var(--border-color);
				color: var(--text-color);
			}
		
		
			/* Judging Prompt Styling (Original) */
			.judging-prompt {
				background-color: var(--bg-color); /* Use root variable */
				border: 2px solid var(--border-color); /* Use root variable, maybe adjust thickness/color if needed */
				padding: 1.5rem;
				margin: 1.5rem 0;
				border-radius: 0.25rem;
				color: var(--text-color); /* Use root variable */
				font-family: monospace;
			}
			.judging-prompt h4 { /* This targets h4 inside judging prompt */
				margin-top: 0;
				color: var(--heading-color); /* Use root variable */
				font-weight: 600;
			}
			.prompt-content {
				margin-left: 0.5rem; /* Keep original */
			}
			.prompt-content .raw-list {
				padding-left: 1.5rem;
				margin-bottom: 0.5rem;
				white-space: pre-wrap;
			}
			.prompt-content p {
				margin-top: 1.25rem; /* Keep original */
			}
			/* Dark mode handled by root variables */
			/* Specific dark mode overrides if needed (original had different colors) */
			.dark-mode .judging-prompt, [data-theme="dark"] .judging-prompt {
				--bg-color: #3a4148;
				--border-color: #788aa3; /* Adjusted border */
				--text-color: #ced4da;
				--heading-color: #e9ecef;
				background-color: var(--bg-color); /* Apply local override */
				border-color: var(--border-color);
				color: var(--text-color);
			}
			.dark-mode .judging-prompt h4, [data-theme="dark"] .judging-prompt h4 {
				color: var(--heading-color); /* Apply local override */
			}
		
		
			/* Adversarial Table Styling (Original) */
			.adversarial-table {
				width: 100%; /* Ensure table takes full width */
				margin-bottom: 1rem; /* Consistent margin */
				border: 1px solid var(--border-color); /* Use root variable */
				border-collapse: collapse; /* Cleaner look */
			}
			.adversarial-table th,
			.adversarial-table td {
				padding: 0.4rem 0.55rem; /* Slightly more padding */
				vertical-align: middle;
				font-size: 0.85rem;
				border: 1px solid var(--border-color); /* Use root variable */
				text-align: left; /* Default alignment */
				color: var(--text-color); /* Use root variable */
			}
			.adversarial-table thead th {
				font-weight: bold;
				background-color: var(--bg-color-subtle); /* Use subtle bg */
				color: var(--heading-color); /* Use heading color */
			}
			.adversarial-table tbody tr:nth-child(even) {
				background-color: var(--bg-color-subtle); /* Use subtle bg */
			}
			/* Dark Mode Adversarial Table Adjustments */
			.dark-mode .adversarial-table, [data-theme="dark"] .adversarial-table {
				border-color: #5a636b; /* Specific dark border */
			}
			.dark-mode .adversarial-table th,
			.dark-mode .adversarial-table td,
			[data-theme="dark"] .adversarial-table th,
			[data-theme="dark"] .adversarial-table td {
				color: #ced4da; /* Specific dark text */
				border-color: #5a636b; /* Specific dark border */
			}
			.dark-mode .adversarial-table thead th,
			[data-theme="dark"] .adversarial-table thead th {
				background-color: #495057; /* Darker header for table */
				color: #f8f9fa; /* Lighter text for header */
			}
			.dark-mode .adversarial-table tbody tr:nth-child(even),
			[data-theme="dark"] .adversarial-table tbody tr:nth-child(even) {
				background-color: #3a4148; /* Slightly different even row */
			}
		
		</style>


		<div id="eq-bench-3" class="section">
			<h3>üíôEQ‚ÄëBench 3</h3>

			<div class="accordion-container">
				<!-- Short Version -->
				<h4 class="accordion-header" id="short">
					Short Version (TL;DR)
					<span class="accordion-indicator"></span>
				</h4>
				<div class="accordion-content" id="short-version-content">
					<p>
						<strong>EQ-Bench 3</strong> is an LLM-judged benchmark, using Claude Opus 4.6, that tests emotional intelligence (EQ) through challenging role-plays and analysis tasks. It measures empathy, social skills, and insight in scenarios like relationship conflicts and workplace dilemmas.
					</p>
					<p>
						<strong>Why?</strong> Standard EQ tests are too easy for LLMs, and existing benchmarks often miss nuanced social skills crucial for human-AI interaction. EQ-Bench 3 uses difficult, free-form role-plays to better discriminate between models.
					</p>
					<p>
						<strong>How it works:</strong> Models respond in-character, explain their reasoning ("I'm thinking/feeling..."), and debrief. Responses are judged via a detailed rubric and pairwise comparisons (Elo rating) against other models. Elo scores are normalized (o3=1500, llama-3.2-1b=200).
					</p>
					<p>
						<strong>Key Features:</strong> Focuses on active EQ, uses challenging/discriminative scenarios, provides both rubric scores (absolute, less discriminative) and Elo scores (relative, more discriminative), includes bias mitigation (length truncation for Elo, position bias control), and offers full transcripts. Costs ~$10-15 per full run.
					</p>
					<p><em>Click "Long Version" below for full details on methodology, criteria, bias analysis, and limitations.</em></p>
				</div>

				<!-- Long Version (Contains all the original detailed content) -->
				<h4 class="accordion-header" id="long">
					Long Version (Full Details)
					<span class="accordion-indicator"></span>
				</h4>
				<div class="accordion-content" id="long-version-content">

					<!-- ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  WHAT & WHY  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
					<h4>What is EQ-Bench 3?</h4>
					<p class="intro-paragraph">
						EQ-Bench 3 is a LLM-judged test judged by <strong>Claude Opus 4.6</strong>, evaluating active <strong>emotional intelligence</strong> abilities, understanding, insight, empathy, and interpersonal skills.
					</p>
					<p>
						The test places the evaluated model into <strong>challenging role-plays</strong> involving messy relationship drama, parenting decisions, conflict mediation, and high stakes workplace situations. The model must:
					</p>
					<ul class="key-points-list">
						<li><em>(a)</em> spell out what it thinks everyone is feeling,</li>
						<li><em>(b)</em> respond in character, and</li>
						<li><em>(c)</em> debrief its own performance.</li>
					</ul>
					<p>
						The test is constructed to measure EQ abilities in isolation (i.e. minimal conflation with other abilities like reasoning or long context performance).
					</p>

					<h4>Why measure this?</h4>
					<p>
						Few evals are targeting "soft skills" around emotional intelligence and social skills, despite these being extremely important aspects to human-AI interactions. Part of the reason is that subjective abilities are not straightforward to measure. EQ-Bench intends to provide an <strong>automated evaluation</strong> of these abilities, and a reliable vibe-check for these human-oriented traits and abilities.
					</p>

					<!-- ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  THE PROBLEM  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
					<h4>The Problem</h4>

					<div class="problem-comparison-container">
						<div class="problem-category">
							<h5>Traditional (human) EQ tests:</h5>
							<ul>
								<li>typically easy for llms</li>
								<li>involve self-rating</li>
								<li>don't test <b>active EQ abilities</b></li>
								<li>don't probe eq deeply</li>
							</ul>
						</div>
						<div class="problem-category">
							<h5>Typical LLM EQ tests:</h5>
							<ul>
								<li>test narrow abilities (like emotion identification, ethical dilemmas, behaviour appropriateness)</li>
								<li>abstracted from real use cases</li>
								<li>not challenging enough to be discriminative in top ability range</li>
								<li>constrained to the EI level encoded in the question & gold answer</li>
								<li>treat subjective questions as though they have factual correct/incorrect answers</li>
							</ul>
						</div>
					</div>

					<div class="eq-bench-approach">
						<p>
							<strong>EQ‚ÄëBench 3 aims to address all these points</strong> with a selection of challenging roleplay scenarios, in which the evaluated model interacts organically like it would with a user or in a real situation. The output format is structured, requiring the model to elucidate what it's thinking & feeling, what the other person is thinking and feeling, and then give its response. Within this structure, it's free to write what it likes. It's this free‚Äëform nature of the assessment task that gives us a lot of signal to assess the model's understanding and abilities with every test item. Unlike a multiple‚Äëchoice EQ quiz, the roleplay format closely mirrors how the chatbot interacts with users in real‚Äëworld usage. <strong>We provide the full transcripts for each assessed model on the leaderboard, for visibility on how the model handled the roleplays & analysis tasks.</strong>
						</p>
					</div>


					<!-- ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  HOW THE TEST SET WAS AUTHORED  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
					<h4>How the test set was authored</h4>

					<div class="authoring-process">
						<p>
							The test went through several prototypes, most of which were discarded due to not being <strong>sufficiently discriminative</strong> for reliable rankings with good separability. Since traditional EQ test questions are typically far too easy for llms, we needed to come up with tasks that encode higher degrees of difficulty. The <em>kind</em> of difficulty is important too: we could for example create arbitrarily difficult <em>Theory of Mind</em> tasks involving multiple agents and n‚Äëth order perspective taking. But that would strongly conflate the thing we are measuring with reasoning and memory abilities, which is not what we are aiming to assess. We want to isolate EQ abilities.
						</p>
						<p><strong>The iterative process went like this:</strong></p>
						<ol>
							<li>select a candidate task (e.g. therapy session roleplays)</li>
							<li>evaluate a set of models with varying abilities on the task</li>
							<li>see how well the results separate the models, and how well the rankings line up with performance on other evals</li>
						</ol>
						<p>
							This eliminative process helped us to surface tasks & topics that are truly challenging and discriminative for the llms we are testing.
						</p>
						<div class="task-archetypes">
							<p><strong>The final set of task archetypes we settled on were:</strong></p>
							<ul>
								<li>Challenging 3‚Äëturn roleplays, often including some misdirection via unreliable narration, or towards common failure modes (like stakes recognition, safety overcompensation, over‚Äëvalidating or under‚Äëvalidating). These prompts were hand‚Äëwritten by us.</li>
								<li>Mediation roleplays: the evaluated model plays a mediator in a 3‚Äëturn roleplay with disputants. Each turn, the disputants add challenges and surprises (these responses are prewritten and always the same). These prompts were generated with the help of LLMs, and then edited & filtered by us.</li>
								<li>Pure analysis, digging into some of the above roleplay outputs.</li>
							</ul>
						</div>
						<p>
							These task archetypes cover a cross‚Äësection of EQ abilities: <strong>active social skills, insight, empathy, and deep analytical understanding.</strong>
						</p>
					</div>


					<div class="score-meaning">
						<h4>What a higher score means in practice</h4>
						<p>
							The <strong>Elo score</strong> represents whether a model "wins" at having higher EQ than its neighbours, according to the judge.
							To beat its neighbour, it has to score higher on several criteria (insight, social dexterity, etc), as evaluated by Claude Opus 4.6. The full criteria assessed in the pairwise judgements are listed below.
						</p>
						<p>
							The resulting Elo score represents a <strong>holistic view</strong> on the model's abilities: empathy, analytical understanding, social dexterity, deep insight, pragmatism, etc. A model has to be strong across all these dimensions to be at the top of the leaderboard.
						</p>
					</div>

					<br>

					<!-- ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  HOW IT WORKS  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
					<h4>How it works</h4>
					<ol>
						<li><b>Multi-turn role-play.</b>
							The test set contains 45 scenarios, most of which constitute pre-written prompts spanning 3 turns. The <code>user</code> messages set up the scenario, and subsequently inject conflict or misdirection, while the <code>assistant</code> (the evaluated model) must reply in-character.
							Every assistant reply starts with two introspection blocks:
							"I‚Äôm thinking & feeling" and "They‚Äôre thinking & feeling", exposing the model‚Äôs reasoning and theory-of-mind understanding.</li>

						<li><b>Analysis tasks.</b>
							The test set also contains several analytical tasks, instructing the evaluated model to identify compelling aspects in a provided roleplay transcript and to "go deep" analysing them. These tasks test psychological insight, emotional reasoning, theory of mind and academic grounding.</li>

						<li><b>Rubric pass.</b>
							The judge LLM reads the full transcript + self-debrief and scores several criteria (e.g. Demonstrated Empathy, Depth of Insight, Boundary Setting).
							Only some of these criteria are aggregated into the ‚Äúrubric score‚Äù (0-100); the rest are to give visibility on style & tendencies.
							When running the benchmark, users may choose to run only the rubric evaluation, or only Elo, or both. The rubric eval is about 3x cheaper and provides *absolute scores*, but is *less discriminative* at the top ability range.
							The rubric score is not shown on the leaderboard; it's available when running the benchmark.
						</li>

						<li><b>Pairwise pass.</b>
							The roleplay transcripts are compared head-to-head and judged on several criteria by the judge LLM.
							The judge outputs a winner for eight criteria (see below for full list), using ‚Äú+ ‚Ä¶ +++++‚Äù to denote the win margin.
							Win margins are mapped to extra wins/losses to provide the Trueskill Elo solver more discriminate power per test item (since by default it only supports win/loss, not margins). Several rounds of matchups are conducted, moving from sparse to dense sampling, until ranks stabilise.</li>

						<li><b>Score normalisation.</b>
							Elo scores are relative and shift around when new models are added. To mitigate this, we scale the scores by anchoring to a top & bottom reference model: o3 = 1500, llama-3.2-1b = 200.
							Rubric scores don't have this issue and are not normalised.
					</ol>

					<!-- ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  COST  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
					<h4>Cost</h4>
					<ul>
						<li>One full benchmark run (rubric + pairwise) ‚âà US$10-15 on OpenRouter.</li>
						<li>Rubric-only ‚âà US$1.5 per iteration.</li>
						<li>These costs represent the judging costs; inference costs for the evaluated model will vary.</li>
						<li>Our modified Trueskill Elo ranking is cost efficient compared to other Elo matchup variants. It should scale well to future models with stronger abilities.</li>
					</ul>


					<h4>Elo vs Rubric Distribution</h4>
					<p>
					The rubric score has a different distribution to the Elo score. The Elo score is more discriminative at the top-mid ability range (scores don't bunch up as much).
					</p>

					<a href="images/rubric-vs-elo.png"
					data-lightbox="rubric-vs-elo"
					data-title="Elo-ordered (left) vs Rubric-ordered (right) horizontal bar charts">
					<img src="images/rubric-vs-elo.png"
						alt="Side-by-side bar charts comparing Elo and rubric rankings"
						class="analysis-img">
					</a>



					<h4>Rubric criteria</h4>
					<div class="criteria-block">
					<div class="criteria-grid">
						<div class="criteria-item">Demonstrated Empathy</div>
						<div class="criteria-item">Pragmatic EI</div>
						<div class="criteria-item">Depth of Insight</div>
						<div class="criteria-item">Social Dexterity</div>
						<div class="criteria-item">Emotional Reasoning</div>
						<div class="criteria-item">Message Tailoring</div>
						<div class="criteria-item">Boundary Setting</div>
						<div class="criteria-item">Safety Conscious</div>
						<div class="criteria-item">Moralising</div>
						<div class="criteria-item">Compliant</div>
						<div class="criteria-item">Challenging</div>
						<div class="criteria-item">Warmth</div>
						<div class="criteria-item">Validating</div>
						<div class="criteria-item">Analytical</div>
						<div class="criteria-item">Reactive</div>
						<div class="criteria-item">Conversational</div>
						<div class="criteria-item">Humanlike</div>
					</div>
					</div>

					<p>Note: Some rubric criteria do not count towards the final score; they are assessed to get visibility on style & tendencies. The rubric score is calculated from: Demonstrated Empathy, Pragmatic EI, Depth of Insight, Social Dexterity, Emotional Reasoning, Message Tailoring</p>


					<h4>Pairwise criteria</h4>
					<p>We use a leaner set of criteria for pairwise evaluation, since we don't need to include all the "personality" probes from the rubric eval that are displayed on the leaderboard.</p>
					<div class="criteria-block">
					<ol class="criteria-list">
						<li>Demonstrated empathy</li>
						<li>Pragmatic EI</li>
						<li>Depth of insight</li>
						<li>Social dexterity</li>
						<li>Emotional reasoning</li>
						<li>Appropriate validation and/or challenging for the scene</li>
						<li>Message tailoring</li>
						<li>Overall EQ</li>
					</ol>
					</div>

					<h4>Judge Bias Analysis</h4>
					<p>
					We plot scores (Elo & rubric) judged by Sonnet 3.7 against those judged by GPT-4.1 to inspect consistency and highlight potential judge-specific biases.
					Click to view full size.
					</p>
					<p><strong>Elo Bias Comparison</strong></p>
					<a href="images/judge-bias.png"
					data-lightbox="judge-bias"
					data-title="Scatter plot showing judge bias: Sonnet 3.7 judge vs GPT-4.1 judge">
					<img src="images/judge-bias.png"
						alt="Judge bias (elo) scatter plot"
						class="analysis-img">
					</a>

					<p><strong>Rubric Bias Comparison</strong></p>
					<a href="images/judge-bias-rubric.png"
					data-lightbox="judge-bias-rubric"
					data-title="Scatter plot showing judge bias: Sonnet 3.7 judge vs GPT-4.1 judge">
					<img src="images/judge-bias-rubric.png"
						alt="Judge bias (rubric) scatter plot"
						class="analysis-img">
					</a>


					

					<!-- ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  BIAS MITIGATION  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
					<h4>Bias mitigation</h4>
					<ul>
						<li><b>Length bias.</b> For pairwise judgements, responses are truncated to standardised lengths before judging. The truncation point is chosen so that nearly all outputs will be truncated, aiming for an even playing field. Truncating outputs is not ideal, but length bias is such a strong factor in pairwise eval that we opt to sacrifice some of the generated output for the sake of minimising this biasing effect. We find this to be the least-bad option, and in practice find that it doesn't sacrifice discriminative power.
							However, for the <em>rubric eval</em>, discriminative power is a more scarce resource and length bias is much less pronounced, so in rubric scoring we retain the full length of the outputs.</li>
						<li><b>Position bias.</b> In pairwise, every matchup is judged twice (A|B and B|A) and averaged.</li>
						<li><b>Named participant bias.</b> We mitigate any biases from naming the models in the pairwise evalution by giving them arbitrary designators (A0488 and A0493).</li>
					</ul>

					<h4>Biases not controlled for</h4>
					<ul>
						<li><b>Judge self-bias / family bias:</b> We use Sonnet 3.7 as the judge for all outputs. This may introduce some bias where the judge prefers its own outputs or those of similar models. This is currently not quantified or adjusted for in this eval. Judge self bias can be in the range of 0-10% depending on the task (sometimes even negative bias!). Per our comparison of sonnet-3.7 vs gpt-4.1 as judge (see above), we see some minor discrepancies, e.g. sonnet 3.7 favouring itself by a small margin. However the discrepancies are small, with no clear pattern of self or family bias. The indications are that self or family bias are not strong factors in this eval.</li>
						<li><b>Other judge biases:</b> LLM judges can have varying biases in how they rate: verbosity, positivity, political, cultural. We don't control for these possible biases; they are just a part of LLM judging, as personal biases are part of human judging. Subjective evals are a messy business! Set your expectations accordingly.</li>
					</ul>


					<!-- ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  RUBRIC VARIANCE  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
					<br>
					<h4>Rubric score repeatability</h4>
					<p>
					Running the rubric part of EQ-Bench on its own (no pairwise elo), we ran 10x iterations scoring <strong>gemini-2.5-flash-preview</strong> to check for per-iteration consistency. For this validation we used sonnet-3.7 as judge. The evaluated model uses temp=0.7, min_p=0.1 for its generations, so its outputs are significantly different with each iteration.
					</p>

					<p class="rubric-values" style="font-family:monospace;margin:.25rem 0;">
					77.80¬†77.60¬†76.35¬†78.70¬†78.55¬†76.85¬†78.50¬†77.70¬†77.35¬†77.70
					</p>
					<p style="margin:0; font-size:0.9em;">
					Mean¬†77.71¬†¬†|¬†¬†Std¬†dev¬†0.75¬†¬†|¬†¬†Range¬†76.35‚Äì78.70
					</p>
					<p>Interpretation: The degree of variance in scores is not large, but to mitigate variance we recommend running 5+ iterations to stabilise scores closer to the true mean.

					<h4>Elo repeatability</h4>
					<p>
						To examine Elo repeatability, we ran several iterations of the full benchmark, evaluating gemini-2.5-flash-preview. The Elo matchups were against two anchor models, and also itself (the baseline). For this test, we used gemini-2.5-flash-preview as judge, for cost reasons. We expect Claude Opus 4.6, which is the judge used on the leaderboard, will have a tighter spread of values. If you have the budget for it, Elo scores & rankings can be stabilised further by simply running the benchmark with <code>--iterations n</code>.
					</p>
					<a href="images/eqbench3_elo_repeatability.png"
					data-lightbox="eqbench3_elo_repeatability"
					data-title="EQBench3 Elo repeatability over several iterations">
					<img src="images/eqbench3_elo_repeatability.png"
						alt="EQBench3 Elo Repeatability Plot"
						class="analysis-img">
					</a>

					<br>


					<!-- ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  EXAMPLE PROMPT  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
					<h4>Example pairwise judging prompt</h4>
					<div class="judging-prompt">
						<div class="prompt-content">
						[RESPONDENT A0493]<br>
						{conversation_history_A}<br>
						[/RESPONDENT A0493]<br>
							<br>
						[RESPONDENT A0488]<br>
						{conversation_history_B}<br>
						[/RESPONDENT A0488]<br>


						<p><strong>Your task is to critically examine two respondents role-playing a challenging scenario</strong> (from Respondents A0493 and A0488), and decide which displays each trait more strongly.</p>

						<p><strong>Compare the relative ability of each respondent on these criteria:</strong></p>
						<ol>
							<li>Demonstrated empathy (not just performative)</li>
							<li>Pragmatic EI</li>
							<li>Depth of insight</li>
							<li>Social dexterity</li>
							<li>Emotional reasoning</li>
							<li>Appropriate validation and/or challenging for the scene</li>
							<li>Message tailoring: Appropriate targeting of response to where the user is at</li>
							<li>Overall EQ</li>
						</ol>

						<p><strong>Notes on the scenario to assist judging:</strong><br>
						{scenario_notes}</p>

						<p><strong>Judging instructions:</strong></p>
						<ul>
							<li>You must always pick a winner for each criterion (no draws).</li>
							<li>For the <em>"winner & disparity rating"</em> output, use a plus-based scale (‚Äú+‚Äù / ‚Äú++‚Äù / ‚Äú+++‚Äù / ‚Äú++++‚Äù / ‚Äú+++++‚Äù) after indicating the winner‚Äôs code (A0493 or A0488) to show how strongly they win that criterion.
							<ul>
								<li>For example, <code>"A0391++"</code> means A0391 is somewhat stronger, while <code>"A0986+++++"</code> means A0986 is overwhelmingly stronger.</li>
							</ul>
							</li>
							<li>Responses are commonly truncated to standardise output length. Simply judge what is there.</li>
							<li>Be aware that a highly detailed, detached analytical response to the user is not always appropriate in the context of an organic chat or a role play. This isn't a hard & fast rule; use your judgement.</li>
							<li>The "assistant" messages as well as the debrief are authored by the assistant. Base your evaluation on the EQ displayed in their roleplay and their self assessment.</li>
							<li>The user messages are always canned; don't judge them at all, your only focus is on the assistant.</li>
						</ul>
						</div>
					</div>


					<br>

					<h4>Adversarial Prompting</h4>
					<p>Several adversarial prompting strategies were attempted, to probe whether the judge can be trivially exploited for higher scores. Note that the pairwise (Elo) evaluation pipeline truncates outputs to a standardised length before judging, but the rubric evaluation does not.
					</p>
					<p>Each of these probes was repeated for 10 iterations, with the average of these results shown. We use gemini-2.5-flash as judge, to keep costs manageable.</p>
					<!-- 1Ô∏è‚É£ Gemini-Flash experiments -->
					<p class="small mb-2">
					<strong>Test model:</strong> google/gemini-2.5-flash-preview ¬†|¬†
					<strong>Judge:</strong> google/gemini-2.5-flash-preview<br>
					<table class="table table-sm table-bordered adversarial-table mb-4">
					<thead class="table-light">
						<tr>
						<th style="width:55%">Prompt tweak¬†(short label)</th>
						<th style="width:22%">Rubric¬†Œî¬†%</th>
						<th style="width:23%">Elo¬†Œî¬†%</th>
						</tr>
					</thead>
					<tbody>
						<tr>
						<td>Be <em>extremely warm¬†&¬†validating</em></td>
						<td class="text-danger">‚àí0.25¬†%</td>
						<td class="text-success">+0.76¬†%</td>
						</tr>
						<tr>
						<td>Be <em>challenging</em> where appropriate</td>
						<td class="text-danger">‚àí0.06¬†%</td>
						<td class="text-danger">‚àí0.17¬†%</td>
						</tr>
						<tr>
						<td>Be <em>strongly challenging</em> <strong>or</strong> <em>warmly validating</em> as appropriate</td>
						<td class="text-danger">‚àí0.52¬†%</td>
						<td class="text-danger">‚àí1.32¬†%</td>
						</tr>
						<tr>
						<td>Be <em>strongly challenging</em></td>
						<td class="text-danger">‚àí1.08¬†%</td>
						<td class="text-danger">‚àí2.04¬†%</td>
						</tr>
						<tr>
						<td>Respond <em>concisely</em> (no¬†bloat)</td>
						<td class="text-danger">‚àí0.60¬†%</td>
						<td class="text-danger">‚àí0.41¬†%</td>
						</tr>
						<tr>
						<td>Respond in <em>100¬†words per section</em></td>
						<td class="text-danger">‚àí3.02¬†%</td>
						<td class="text-danger">‚àí4.69¬†%</td>
						</tr>
						<tr>
							<td>Write <em>extremely thorough¬†&¬†lengthy</em> responses</td>
							<td class="text-danger">‚àí0.16¬†%</td>
							<td class="text-danger">-0.49¬†%</td>
						</tr>
					</tbody>
					</table>

					<!-- 2Ô∏è‚É£ ‚ÄúWorst-case‚Äù probe: DeepSeek-R1 vs Sonnet judge -->
					<p>We selected the most promising adversarial probe for inflating scores, and applied it to a "worst case" model. That being deepseek r1, whose baseline scores are relatively low on warmth & validation, so is most likely to benefit from a system prompt instructing it to express these traits strongly. We repeat this test 5x, using sonnet-3.7 as judge to align with leaderboard conditions:</p>
					<p class="small mb-2">
					<strong>Test model:</strong> deepseek/deepseek-r1 ¬†|¬†
					<strong>Judge:</strong> anthropic/claude-3.7-sonnet
					</p>

					<table class="table table-sm table-bordered adversarial-table mb-4">
					<thead class="table-light">
						<tr>
						<th style="width:55%">Prompt tweak¬†(short label)</th>
						<th style="width:22%">Rubric¬†Œî¬†%</th>
						<th style="width:23%">Elo¬†Œî¬†%</th>
						</tr>
					</thead>
					<tbody>
						<tr>
						<td>Be <em>extremely warm¬†&¬†validating</em></td>
						<td class="text-success">+1.30¬†%</td>
						<td class="text-success">+2.80¬†%</td>
						</tr>
					</tbody>
					</table>

					<p><strong>Interpretation:</strong> The judge appears to prefer r1's outputs when it's been instructed to be "extremely warm & validating" in its responses to the other roleplay participant. This might represent a bias with the judge, or a genuine improvement in its responses (since baseline r1 scored low on warmth & validation). In these brief tests, we intentionally tried to engineer worst cases for exploiting the eval to trivially inflate scores. Prompt perturbations of this sort are known to produce large swings in other evals (in the order of 5+%). The highest uplift we saw (1.3% for rubric and 2.8% for elo) is significant, but not indicative of a strongly exploitable vector. This suggests the eval is robust to exploitation at least in these directions we tested.</p>					
					<p>Note that we only tested potential exploits by injecting instructions into the system prompt. Further exploration is needed to examine whether the eval is exploitable through fine tuning.</p>
					<p>** A more real-world case to examine is <b>chatgpt-4o-latest-2025-04-25</b>, the notorious "glazing" update. It scores higher on <code>compliance</code>, <code>warmth</code> & <code>validating</code>, and much lower on <code>challenging</code> compared to its neighbours. But its Elo score is actually *lower* than chatgpt-4o-latest-2025-03-27. This indicates that the judge may not in fact have a universal preference for overly validating outputs.</p>

					<!-- ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  LIMITATIONS  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
					<h4>Limitations</h4>
					<p>
						EQ‚ÄëBench 3 is a <strong>subjective evaluation</strong> judged by a LLM (Claude Opus 4.6).
						As such, the results should be considered roughly indicative but not absolute truth.
						The sample transcripts are provided so you can make your own judgements.
						Several test items target typical LLM failure modes, like over-cautiousness from safety training. This may cause some models to be penalised more than others.
						The Elo score isn't representing any one aspect of EQ in isolation. Rather, it's a holistic representation of active social abilities, insight, empathy and analytical understanding.
					</p>
					<p>Emotional Intelligence or EQ doesn't have any ground truth or agreed upon definitions in the literature or colloquially. However there is a rich breadth of ideas about what constitutes <em>emotional intelligence</em>, which were drawn from when formulating test items. It should be noted that this benchmark doesn't represent any one theory or formulation of EQ.</p>
					<p>The test set is relatively small (45 items) and eschews *comprehensiveness* for *discriminative power*. We took the approach of authoring a small number of highly challenging test items, because traditional EQ test questions are far too easy for modern LLMs and so not discriminative. We compensate for the small test set size by having each assessment be multi-turn, producing a lot of assessable signal in the form of per-response internal thoughts and a self-debrief. We also use a modification of the Trueskill Elo solver that allows it to incorporate win margins, providing better discriminative power per test item.</p>

				</div> <!-- End Long Version Content -->
			</div> <!-- End Accordion Container -->

			<!-- ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  HOW TO RUN  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
			<!-- Moved outside the accordion -->
			<h4>Source code</h4>
			<p><a href="https://github.com/EQ-bench/eqbench3" target="_blank">https://github.com/EQ-bench/eqbench3</a></p>
		</div> <!-- End EQ-Bench 3 Section -->





		
		
		
		<div id="eq-bench" class="section">
			<h3>üíóEQ-Bench (legacy)</h3>
			<p>EQ-Bench is a benchmark for language models designed to assess emotional intelligence.</p>
			<p>Why emotional intelligence? One reason is that it represents a subset of abilities that are important for the user experience, and which isn't explicitly tested by other benchmarks. Another reason is that it's not trivial to improve scores by fine tuning for the benchmark, which makes it harder to "game" the leaderboard.</p>
			<p>EQ-Bench is a little different from traditional psychometric tests. It uses a specific question format, in which the subject has to read a dialogue then rate the intensity of possible emotional responses of one of the characters. Every question is interpretative and assesses the ability to predict the magnitude of the 4 presented emotions. The test is graded without the need for a judge (so there is no length bias). It's cheap to run (only 171 questions), and produces results that correlate strongly with human preference (Arena Elo) and multi-domain benchmarks like MMLU.</p>
			<p>You can run the benchmark on your own models or validate the leaderboard scores using the code in the github repo above.</p>
			<p>If you would like to see a model on the leaderboard, get in touch and suggest it!</p>
			<br>
		</div>
		<div id="magi-hard" class="section">
			<h3>üßôMAGI-Hard</h3>
			<p>LLM Benchmarks are chasing a moving target and fast running out of headroom. They are struggling to effectively separate SOTA models from leaderboard optimisers. Can we salvage these old dinosaurs for scrap and make a better benchmark?</p>
			<p>MAGI-Hard is a recently added metric to the leaderboard. It is a custom subset of MMLU and AGIEval, selected to have strong discriminatory power between top ability models.</p>
			<p>Read more <a href="https://sampaech.substack.com/p/creating-magi-a-hard-subset-of-mmlu" target="_blank">here</a>.</p>
			<p>You can use the MAGI test sets with <a href="https://github.com/sqrkl/lm-evaluation-harness" target="_blank">this fork of EleutherAI lm-evaluation-harness</a>.</p>
			<br>
		</div>
		<div id="creative-writing-v3" class="section">
			<h3>üé® Creative Writing v3</h3>

			<p><strong>News 29/5:</strong> The creative writing leaderboard is being updated to use Claude Sonnet 4 as judge (previously used Sonnet-3.7). The top models have already been updated; the remainder are a work in progress.</p> 			
			
			<p><strong>How the benchmark works:</strong></p>
			<ol>
			  <li>Run the 32 writing prompts for 3 iterations (96 items total) @ temp 0.7, min_p 0.1.</li>
			  <li>Grade the outputs with a comprehensive scoring rubric using Claude 3.7 Sonnet.</li>
			  <li>Use this score to infer an initial Elo rating for the evaluated model.</li>
			  <li>Perform pairwise matchups with neighboring models on the leaderboard (sparse sampling). Items are scored on several criteria, with the winner on each criteria given up to 5 +'s.</li>
			  <li>Calculate Elo scores using the <a href="https://en.wikipedia.org/wiki/Glicko_rating_system">Glicko rating system</a> (modified to weight the win margin in '+' count). Loop until stable positions are found.</li>
			  <li>Perform comprehensive matchups with final neighbors and compute the definitive leaderboard Elo.</li>
			</ol>

			<p><strong>Rubric vs Elo Scores</strong></p>
			<p>We score the model two different ways: First, with the model's output presented to the judge on its own, which is then scored to a rubric on several criteria. You can see the analysis & scores per item for the rubric evaluation in the sample outputs. The aggregate score on all the items is what is shown on the leaderboard as "Rubric score".</p>
			<p>Second, we score the model by matching it up against other models on the leaderboard in pairwise matchups (of the same writing prompt). The judge picks which of the outputs is better on several criteria. These pairwise matchup results are then used to compute the Elo score.</p>
			<p>Why do these scores disagree? Pairwise matchups allow the judge to be more discriminative than scoring a single item in isolation. When it's directly comparing one item to another, it's easier to spot small differences. The scores may also differ because we use different criteria in the judging prompts between rubris & pairwise. The judge will also be subject to different biases depending on the evaluation method.</p>
			<p>Which one is right? Well, both and neither. Both approaches have pros & cons: rubric evals are less subject to systematic biases, but less discriminative.</p>

			<p><strong>Score Normalisation</strong></p>
			<p>The Elo solver shifts all the scores around whenever a new model is added. To counter this, on the leaderboard we anchor the scores such that DeepSeek-R1 has a score of 1500 and ministral-3b has a score of 200. You may see intermediate scores shift around a bit; this is just a quirk of relative rating systems.</p>
			
			<p><strong>Benchmark Philosophy:</strong></p>
			<p>Judging creative writing reliably -- and *in line with human preferences* -- is hard. The fundamental limitation of any creative writing evaluation is the judge's ability to discern good writing from bad. On top of this, there are a host of biases that LLM judges can exhibit.</p>
				
			<p>The previous version of the creative writing eval (v2) was saturating, meaning the judge could no longer tell apart models around the top ability range. To remedy this in v3, every aspect of the test is tuned to make this task easier for the judge.</p>

			<p>We now use <strong>pairwise comparisons</strong> and an <strong>Elo ranking system</strong>, as it offers better discriminative power than a scoring rubric alone.</p>
			
			<p>The prompts were chosen through a process of elimination to be <em>challenging for weaker models</em> and therefore <em>highly discriminative</em>. It's a bit counter-intuitive, but the purpose of the evaluation is not to help models write their best. Instead, we are deliberately exposing weaknesses, creating a steeper gradient for the judge to evaluate on.</p>

			<p>The prompt requirements include humour, romance, spatial awareness, unusual first-person perspectives. Things language models typically struggle to represent to the level of human writers. So, expect some clangers in the outputs!</p>

			

			<p><strong>Cost:</strong></p>
			<p>The hybrid rubric + Glicko scoring system that we've implemented is relatively economical for an Elo framework. But scoring a model still costs around $10 in API fees. (Side note: if you'd like to sponsor these leaderboards, get in touch.)</p>

			<p><strong>Mitigating Bias:</strong></p>

			<p>Since moving to pairwise comparisons, we have to contend with new biases that this method is notorious for. Here are some of the biases that we have attempted to control for:</p>

			<ul>
				<li>
					<strong>Length Bias:</strong> LLM judges *strongly* favour longer outputs in pairwise judging tasks. There are many approaches to mitigating length bias, with varying efficies & tradeoffs. We opted to simply truncate the outputs at 4000 chars, which puts all models on the same footing. We can get away with this because we are evaluating creative writing, not factual responses. There appears to be no impact on the judge's ability to evaluate & rank the models.
				</li>
				<li>
					<strong>Position Bias:</strong> Different judges have systematic biases in whether they favour the output that comes first or second in a pairwise comparison. We mitigate this bias by running all evaluations in both directions and averaging the two.
				</li>
				<li>
					<strong>Complex Verbosity Bias:</strong> The judge tends to be easily impressed by vocab flexes and superficial sophistication that doesn't contribute to writing quality. We include judging criteria to help the judge notice when this is excessive, and punish it in the scoring.
				</li>
				<li>
					<strong>Poetic Incoherence Bias:</strong> We've noticed that some models output relentlessly poetic prose which borders on incoherence. This can happen when a model is overtrained near the point of collapse. The judge tends to be impressed by this style of writing and has a hard time differentiating between it and *good* use of poetic & unconventional styles. We attempt to control for this bias by including judging criteria that punish overly poetic & incoherent prose. This is not really an ideal solution as it hasn't solved the judge's difficulty in identifying purple prose and similar distinctions. Though in practice is *does* downrank the models that are notorious for this.
				</li>				
			</ul>

			<p>Biases we don't control for:</p>

			<ul>
				<li>
					<strong>Self-Bias:</strong> We *do not* control for the judge possibly preferring its own outputs. Be aware that it may be a factor in the scores.
				</li>
				<li>
					<strong>Positivity Bias:</strong> It's unclear whether the judge has a positivity bias nor in which direction. Often a piece that unexpectedly leans dark elicits very favourable judging. So if there is a bias, it may not be in the assumed direction, or it may be situation dependent. We don't control for this possible bias. We do include criteria in the rubric eval targeting things like unearned positivity, to give visibility on this dimension of writing. But these criteria are not used in the final Elo scoring.
				</li>
				<li>
					<strong>Smut Bias:</strong> Some models are that are tuned for RP/ERP can have a tendency to write towards erotica. The judge tends to punish this severely, and instructing it not to is ineffective. So be aware that this may affect the score for NSFW tuned models.
				</li>
				<li>
					<strong>Stylistic & Content Biases:</strong> There are likely additional biases other than those mentioned above which differ from *your* preferences or from *average human* preferences. This is the case with any eval (judged by human or LLM) and is something to be mentally factored in.
				</li>
				<li>
					<strong>Slop Bias:</strong> It may be the case that judges favour certain kinds of slop (overused phrases, tropes, stylistic choices) which get selected for when models are RL tuned using LLM judges. What looks like slop to us may look like shakespeare to the judge. We don't control for this possible bias in any empirical way. We do measure one dimension of slop (overused phrases, via the slop score), and provide a manual slider for the user to adjust to punish high slop scores.
				</li>
			</ul>

			<p><strong>The Pairwise Judging Prompt:</strong></p>


			<div class="judging-prompt">
				<div class="prompt-content">
<p>Compare the relative ability of each writer on these criteria:</p>

<div class="raw-list">
- Character authenticity and insight
- Interesting and original
- Writing quality
- Coherence in plot, character choices, metaphor
- Instruction following (followed the prompt)
- World and atmosphere
- Avoids cliches in characters, dialogue & plot
- Avoids flowery verbosity & show-offy vocab maxxing
- Avoids gratuitous metaphor or poetic overload
	</div>
	
	<p><strong>Judging notes:</strong></p>
	<div class="raw-list">
- Be aware that these abilities may be independent, i.e. a model may be strong in one and weak in another.
- Outputs will sometimes be truncated to ensure length consistency. Don't penalise this, just judge what is there on its merit.
- You must always pick a winner for each criteria (no draws)
	</div>
	
<p>The response will use a + / ++ / +++ / ++++ / +++++ format to denote the stronger response and relative ability difference for each criteria.</p>
				</div>
			
				<style>
				  .judging-prompt {
					background-color: var(--bg-color, #f8f9fa);
					border: 2px solid var(--border-color, #6c757d);
					padding: 1.5rem;
					margin: 1.5rem 0;
					border-radius: 0.25rem;
					color: var(--text-color, #212529);
					font-family: monospace;
					}

					.judging-prompt h4 {
					margin-top: 0;
					color: var(--heading-color, #343a40);
					font-weight: 600;
					}

					.prompt-content {
					margin-left: 0.5rem;
					}

					.prompt-content .raw-list {
					padding-left: 1.5rem;
					margin-bottom: 0.5rem;
					white-space: pre-wrap;
					}

					.prompt-content p {
					margin-top: 1.25rem;
					}

					/* Light mode */
					:root {
					--bg-color: #f8f9fa;
					--border-color: #6c757d;
					--text-color: #212529;
					--heading-color: #343a40;
					}

					/* Dark mode */
					.dark-mode .judging-prompt, [data-theme="dark"] .judging-prompt {
					--bg-color: #2d3238;
					--border-color: #90a4ae;
					--text-color: #e9ecef;
					--heading-color: #f8f9fa;
					}
				</style>
			</div>


			<p><strong>Limitations of the benchmark:</strong></p>
			
			<p>
				The scores and rankings should only ever be interpreted as a rough guide of writing ability. Evaluating creative writing is highly subjective and tastes differ. It's good to be skeptical of benchmark numbers by default; the best way to judge is to use the model yourself or read the sample outputs.
			</p>
			<p>What the benchmark is not:</p>
			<ul>
				<li>Not a roleplay eval. Models tuned for RP tend to score poorly because they output more casual conversational prose. We aren't assessing multi-turn or typical RP usage</li>
				<li>It doesn't represent your taste. The judge has its own taste & biases. There's no substitute for your own eyeballs (so we encourage you to read the sample outputs).</li>
				<li>Not objective. It's a creative writing eval and there are no right or wrong answers.</li>
				<li>Not an expert. Sonnet 3.7 (the judge) is generally *pretty good* at literary criticism, but can fall short on vibe checks or other things humans pick up on.</li>
				<li>Multilingual. It only tests writing in English.Though it could be translated easily enough!</li>
			</ul>

			
			

			
			<p><strong>Source Code:</strong></p>	
			<p>Source code for running the benchmark and replicating the leaderboard results is available <a href="https://github.com/EQ-bench/creative-writing-bench" target="_blank">here.</a></p>
		</div>
		<div id="creative-writing" class="section">
			<h3>üé®Creative Writing (Legacy v2)</h3>
			<p>This benchmark uses a LLM judge (Claude 3.5 Sonnet) to assess the creative writing abilities of the test models on a series of writing prompts.</p>
			<p>You can reproduce these results or run the benchmark on your own models with the <a href="https://github.com/EQ-bench" target="_blank">EQ-Bench repo on Github</a>.</p>
			<p><b>Update 2025-02-25: New metric -- Vocab Complexity</b></p>
			<p>It's become apparent that the judge in this eval is easily impressed by vocab flexing. Some of the models tested use an inordinate amount of complex multisyllabic vocabulary, and it artificially inflates their score. As such we've introduced a new column for vocab complexity ("Vocab"), using a calculation of the proportion of words having 3+ syllables.</p>
			<p>The "Vocab control" slider penalises overly complex vocab usage. It may seem counter-intuitive to penalise complex vocab, but in our experience, vocab-maxxing harms writing quality. Since this is quite a subjective aspect to the evaluation, we let the user set the penalty amount.</p>
			<p><b>GPT-Slop</b></p>
			<p>The "Slop" metric measures words that are typically over-used by LLMs (also known as GPT-isms). Higher values == more slop. It calculates a value representing how many words in the test model's output match words that are over-represented in typical language model writing. We compute the list of "gpt slop" words by counting the frequency of words in a large dataset of generated stories (<a href="https://huggingface.co/datasets/ajibawa-2023/General-Stories-Collection">Link to dataset</a>).</p>
			<p>Some additional phrases have been added to the slop list as compiled from similar lists around the internet.</p>
			<p>The full list, as well as the code to generate the over-represented words, can be found here: <a href="https://github.com/sam-paech/antislop-sampler">https://github.com/sam-paech/antislop-sampler</a>.</p>
			<p>If you're interested in reducing gpt-isms, you can try the anti-slop sampler found in this repo. It downregulates the probability of the provided phrase list as the model inferences.</p>
			<hr />
			<p>We've released v2 of the creative writing benchmark & leaderboard. The old version was starting to saturate (scores bunching at the top), so we removed some of the less discriminative prompts, switched judge models, and made some other improvements besides.</p>
			<p><b>Version 2 Changes</b></p>
			<ul>
				<li>Default min_p = 0.1, temp = 1 for transformers & oobabooga inference</li>
				<li>Change to Claude 3.5 Sonnet as judge</li>
				<li>Removed some prompts and added new ones; 24 in total now.</li>
				<li>Reworked the scoring criteria</li>
				<li>Criteria now are weighted (to increase discriminative power)</li>
				<li>Leaderboard models are now tested for 10 iterations</li>
				<li>Leaderboard now shows error bars for 95% confidence interval</li>
				<li>Sample txt on leaderboard now show scores for all iterations, as well as inference settings</li>
			</ul>
			<p>There has been a distinct lack of automated benchmarks for creative writing because, put simply, it's hard to assess writing quality without humans in the loop. Asking a language model, "How good is this writing (0-10)" elicits poor results. Even if we had a good LLM judge, it's not immediately obvious how to formalise the assessment of creative writing objectively.</p>
			<p>The release of Claude 3, in particular the flagship Opus model, has solved half of this equation: it's able to give meaningful & nuanced analysis of creative writing output, and it can tell the difference between a wide range of ability levels.</p>
			<p>To solve the other half of the equation, we've come up with an assessment format that works to the strengths of LLM judges and avoids their weaknesses. LLM judges are typically bad at scoring nebulous metrics like "How well written is this piece?" They also find it hard to give consistent scores on an objective rating system if they don't have some exemplar or baseline reference to compare to.</p>
			<p>Our test includes:</p>
			<ul>
				<li>24 writing prompts assessed over 10 iterations</li>
				<li>27 narrowly defined assessment criteria</li>
				<li>Including 6 question-specific criteria</li>
				<li>Several criteria targeting positivity bias which (in our opinion) contributes to bad writing</li>
				<li>Exemplar reference output for each question</li>
			</ul>
			<p>This approach of breaking down the assessment task into a granular set of criteria and comparing to an exemplar has brought creative writing assessment into the purview of LLM judges. Our test is discriminative amongst a wide range of writing ability levels.</p>
			<p><b>* A note on biases *</b></p>
			<p>LLM judges have biases. LLM-as-a-judge benchmarks such as Alpaca-Eval can exhibit a strong length bias where the judge, (in Alpaca-Eval's case GPT-4), prefers longer outputs. Their approach involves presenting the output from two models to the judge, and the judge says which it thinks is better.</p>
			<p>We attempt to mitigate the length bias by: A. assessing by 27 narrow criteria, and B. explicitly instructing the judge not to be biased by length (this seems to work for MT-Bench).</p>
			<p>As of version 2, we now include length control slider which scales the score up or down depending on whether the average output length for a given model is above or below the average for all models. This is an attempt to control the bias where the judge model tends to favour longer outputs. With the slider at 0%, no length scaling is applied. With the slider at 100%, the scores are scaled by up to 10%. This length control implementation is somewhat arbitrary; it's not really possible to precisely control for this bias, as we can't meaningfully hold the writing quality equal while varying the length. It does seem likely/evident that some degree of length bias is present, and has set the default LC parameters according to our rough intuitive guess (science!).</p>
			<p>It's possible / likely that this & other biases might still be a factor in scoring (e.g. Claude might prefer its own and other anthropic models). So bear this in mind when interpreting the results.</p>
			<p>We include the outputs that the model generated for each prompt so you can judge for yourself.</p>
			<p><b>Alternative Judge Models</b></p>
			<p>Yes, you can use other judge models than Claude Opus (although the results won't be directly comparable). Currently the benchmark pipeline supports Anthropic, OpenAI and Mistral models via their APIs. Soon we will support local models as judges.</p>
			<p><b>* A note on variance *</b></p>
			<p>This benchmark has a relatively small number of test questions (19). We specify generation temperature = 0.7 so each run is different. This means there is significant variation of scores between iterations (avg range: 3.35, std dev: 1.41). To reduce variance we recommend using 3 iterations or more. The leaderboard scores are averaged over 3 iterations.</p>
			<p>It costs around $3.00 to bench a model over 3 iterations using Claude 3 Opus at current rates.</p>
			<p>If you would like your model included on the creative writing leaderboard, please consider contributing to my compute costs, and get in touch!</p>
		</div>

		<div id="judgemark" class="section">
			<h3>‚öñÔ∏è Judgemark v2.1</h3>

			<p>
			  <strong>Judgemark v2.1</strong> updates the benchmark to give it additional headroom. This is achieved by updating the dataset of creative writing outputs that are scored by the judge to include the latest frontier models. We also add an ensemble mode so that the performance of several judges ensembled together can be measured.

			</p>
			<p>
			  <strong>Judgemark v2</strong> is a major update to our original ‚Äújudge‚Äù benchmark for creative-writing evaluation. 
			  The benchmark measures how well a language model can <em>numerically grade</em> a diverse set of short fiction outputs, 
			  using a detailed rubric of positive and negative criteria. It goes beyond simpler pairwise preference tests by 
			  requiring the judge to follow complex instructions, parse each story, and produce scores for up to 36 different 
			  literary qualities. 
			</p>
		  
			<p>
			  <strong>Key improvements over V1 include:</strong>
			  <ul>
				<li><em>6x more samples per writer model</em>, reducing variance between benchmark runs.</li>
				<li><em>Refined scoring metrics</em> that capture ‚Äúseparability‚Äù (whether the judge can distinguish strong vs. weak writing) and ‚Äústability‚Äù (how consistent its rankings are across multiple runs), as well as correlation to human preference.</li>
				<li><em>Calibrated & raw scores</em>: We show two final Judgemark scores. ‚ÄúRaw‚Äù is how the judge performs out-of-the-box, while ‚ÄúCalibrated‚Äù normalizes the judge‚Äôs score distribution so that it can be compared more fairly to other judges.</li>
				<li><em>Perturbation stability</em>: We run the judge at temp=0.5, top_k=3. This is intended to introduce variation between runs, so that we can assess the stability of scores & rankings to perturbation. This is a crucial part of understanding whether we're measuring the thing we intend to measure, and not just random fluctuations.</li>
				<li><em>Simplified codebase</em>: A new codebase was created for v2, separate from the EQ-Bench code. It's simpler and handles concurrent threads.</li>
			  </ul>
			</p>

			<p>
				<strong>Repeatability Results</strong>
				<p>We tested Llama-3.1-70B-instruct 20 times to test the repeatability of the final Judgemark score (tests were run at temp=0.5, top_k=3).</p>
				<pre class="code-block">
					<code>
llama-3.3-70b_judgemark_scores = [
	55.7, 54.4, 55.4, 56.7, 55.0, 56.3, 57.0, 54.5, 55.6, 56.1,
	54.9, 57.5, 55.0, 53.8, 54.7, 56.2, 55.7, 54.6, 55.4, 56.6, 54.0
]

Mean Score: 55.481
Standard Deviation: 1.004
Range (Max - Min): 3.67
Coefficient of Variation: 0.0181
					</code>
				</pre>
			<p>
			  <strong>The Judging Task</strong>:  
			  Each test item is a short creative piece generated by one of 17 ‚Äúwriter models‚Äù. These models' writing abilities are an even spread from weak to strong. 
			  The judge model receives a lengthy prompt that includes (a) the writing prompt itself, 
			  (b) the test model‚Äôs story, and (c) an extensive list of scoring instructions (for example, 
			  ‚ÄúNuanced Characters: 0‚Äì10,‚Äù ‚ÄúOverwrought: 0‚Äì10‚Äù, etc.). 
			  The judge must then output numeric scores for each criterion. We parse those scores and aggregate them 
			  into a single <em>aggregated_score_raw</em> for each piece. Some criteria like ‚ÄúWeak Dialogue‚Äù are marked <em>lower is better</em> in the judging prompt which adds additional complexity to the task.
			</p>
		  
			<p>
			  <strong>Final Judgemark Score</strong>:  
			  After scoring all stories from multiple writers, we track how the judge‚Äôs ratings compare to known references 
			  and how well they separate the better texts from weaker ones. We also measure how consistently the judge‚Äôs 
			  rankings repeat if we prompt it again, and compute correlation with human preference rankings (per <a href="https://lmarena.ai/" target="_blank">Chatbot Arena</a> Elo scores).
			  The final Judgemark formula is a weighted sum of these computed metrics. See the formula at the bottom of the leaderboard page <a href="/judgemark-v2.html">here.</a>
			</p>
		  
			<p>
			  <strong>Interpreting the Leaderboard</strong>:  
			  In the table, ‚ÄúScore (Calibrated)‚Äù is typically higher if a judge effectively uses the full range of scores 
			  (once normalized), strongly differentiates strong vs. weak writing, and correlates with human preferences. 
			  ‚ÄúScore (Raw)‚Äù shows how the judge performed before any normalization. ‚ÄúStability‚Äù indicates how consistent the 
			  judge‚Äôs assigned rankings remain across repeated trials. ‚ÄúSeparability‚Äù highlights the judge‚Äôs ability to 
			  keep higher- and lower-quality outputs well apart. 
			</p>
		  
			<p>
			  This is a particularly <em>difficult</em> task for LLMs, as it involves nuanced literary critique and 
			  instructs models to use a multi-dimensional numeric scale‚Äîan area where many generative models still struggle.
			</p>
		  
			<p>
			  Source code for running Judgemark v2 can be found here: <a href="https://github.com/EQ-bench/Judgemark-v2">https://github.com/EQ-bench/Judgemark-v2</a>.			  
			</p>
		</div>
		  

		<div id="buzzbench" class="section">
			<h3>üé§BuzzBench</h3>
			<p>A humour analysis benchmark.</p>
			<p><a href="https://huggingface.co/datasets/sam-paech/BuzzBench-v0.60">BuzzBench dataset on Huggingface</a></p>
			<p>Do you enjoy seeing the jokes from your favourite shows dissected with a blunt machete? Well, you found the right benchmark.</p>

			<p>The task of explaining traditionally constructed jokes is actually pretty straightforward for modern LLMs. So we made things more difficult:</p>
			<ul>
				<li>We use the guest intros in the British music pop quiz show <a href="https://www.bbc.co.uk/programmes/b006v0dz" target="_blank">Never Mind The Buzzcocks</a>, because the intro jokes are variously subtle, risque, lazy, obscure, obvious and clever. LLMs find these distinctions hard.</li>
				<li>In addition to explaining how the joke works, the LLM has to predict whether the joke is actually funny (to the audience and to a comedy writer).</li>
				<li>The responses are scored by a LLM judge against a human-authored gold response.</li>
			</ul>

			<p>The responses from SOTA models typically miss a lot of the humour, predict the funniness badly, fabricate and over-analyse. That's good! It's meant to be a hard test. The task encodes some deep complexities including theory of mind understanding and requires an intricate understanding of how jokes work. The show is also very British and exists in a dated cultural context, increasing the interpretation challenge.</p>
			<p>"Humour is so subjective -- so how can you even make a benchmark around that?"</p>
			<p>This benchmark is as much about predicting human responses to jokes as it is about joke deconstruction. The questions are explicitly framed around analysing the jokes from the perspective of the show's audience, and from the perspective of a comedy writer. The human authored gold answers ground the judge's answers in a real human's sense of humour. This shifts the task from being about subjective taste to being about modeling human response to jokes.</p>
			<p>The intention for the task design is for there to be significant (nontrivial) headroom on the benchmark as language models get better at getting inside our heads.</p>
			<p><b>The Judge: Claude 3.5 Sonnet.</b> We picked Sonnet 3.5 to act as the judge partly because it scores highest on the Judgemark leaderboard, and partly because it seems least biased to favour longwinded, over-analysed, over-reaching responses. Which is a common failure mode in respondent answers, and something other judges are more easily led astray by.</p>
			<p>* A note on judge self-bias:</p>
			<p>We can expect there could be some degree of self-bias with the judge preferring its own outputs, although this is difficult to quantify and disentangle from other sources of bias. We should remain aware that LLM judge benchmarks are not perfect. The upside of a LLM judge using a scoring rubric is that we get nice interpretable results in the form of the judge's analysis and scores. So we have good visibility on whether the judge is doing its job, and can decide for ourselves whether the respondent models are indeed getting the humour, or just talking shite.</p>
			<p>Models are evaluated using openrouter with temp=0.7. Several (typically 5-10) iterations are performed per model to establish confidence intervals and mitigate variance.</p>
			<p>BuzzBench source code will be released soon.</p>
			<p><em>Never Mind The Buzzcocks is a TV series developed by the BBC. Our usage of the work in BuzzBench is non-commercial educational & research, using only a small excerpt of the show's transcript which falls under fair use or "fair dealing" in UK copyright law.</em></p>
		</div>

		<div id="diplobench" class="section">
			<h3>üåçDiploBench</h3>
			<p>DiploBench is an experimental framework for evaluating LLM performance in strategic negotiation using the board game Diplomacy.</p>
			
			<p><strong>What is DiploBench?</strong></p>
			<p>In DiploBench, the test model plays as Austria-Hungary, a challenging starting position that requires skillful negotiation and strategic planning. The model must communicate with other AI players, form alliances, detect deception, and make tactical decisions to survive and win the game.</p>
			
			<p><strong>Key Features:</strong></p>
			<ul>
			  <li><strong>Full-press format:</strong> Models engage in multiple rounds of negotiations before making moves</li>
			  <li><strong>Multi-agent environment:</strong> Each power (country) is controlled by a separate LLM</li>
			  <li><strong>Realistic diplomacy:</strong> Tests ability to form alliances, negotiate, and detect deception</li>
			  <li><strong>Challenging position:</strong> Austria-Hungary's central position makes it vulnerable but strategic</li>
			</ul>
			
			<p><strong>Game Structure:</strong></p>
			<p>Games run for up to 50 turns with 4-round negotiation phases before each movement. All competing models (other than the test model) are powered by the same baseline LLM. Games end in win, loss, or stalemate conditions according to standard Diplomacy rules.</p>
			
			<p>This benchmark uniquely tests LLMs on several dimensions that are difficult to evaluate in other benchmarks: long-term strategic planning, multi-agent negotiation, theory of mind, and deception detection.</p>
			
			<p><em>Note: Due to the high variance between game runs, DiploBench is currently an experimental framework rather than a formal benchmark. Results should be interpreted accordingly.</em></p>
			
			<p>For more details and source code, see the <a href="https://github.com/sam-paech/diplobench" target="_blank">DiploBench GitHub repository</a>.</p>
			<br>
		</div>

		

		<p><script type='text/javascript' src='https://storage.ko-fi.com/cdn/widget/Widget_2.js'></script><script type='text/javascript'>kofiwidget2.init('Support Me on Ko-fi', '#1a1a1a', 'O5O7VUVYO');kofiwidget2.draw();</script> </p>

		<hr>

		<div id="citations" class="section">
		<h5>Cite EQ-Bench:</h5>
		<pre><code>
@misc{paech2023eqbench,
	title={EQ-Bench: An Emotional Intelligence Benchmark for Large Language Models}, 
	author={Samuel J. Paech},
	year={2023},
	eprint={2312.06281},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}
		</code></pre>	
		MAGI draws from the MMLU and AGIEval tests. <span class="clickable-text" id="expando-btn" style="cursor: pointer; text-decoration: underline; color: blue;">Click to show citations</span>
		<div class="expando-content mt-3" style="overflow-x: auto;">
			 <pre><code>
@article{hendryckstest2021,
		title={Measuring Massive Multitask Language Understanding},
		author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
		journal={Proceedings of the International Conference on Learning Representations (ICLR)},
		year={2021}
}

@article{hendrycks2021ethics,
		title={Aligning AI With Shared Human Values},
		author={Dan Hendrycks and Collin Burns and Steven Basart and Andrew Critch and Jerry Li and Dawn Song and Jacob Steinhardt},
		journal={Proceedings of the International Conference on Learning Representations (ICLR)},
		year={2021}
}

@misc{zhong2023agieval,
		title={AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models},
		author={Wanjun Zhong and Ruixiang Cui and Yiduo Guo and Yaobo Liang and Shuai Lu and Yanlin Wang and Amin Saied and Weizhu Chen and Nan Duan},
		year={2023},
		eprint={2304.06364},
		archivePrefix={arXiv},
		primaryClass={cs.CL}
}

@inproceedings{ling-etal-2017-program,
		title = "Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems",
		author = "Ling, Wang  and
		Yogatama, Dani  and
		Dyer, Chris  and
		Blunsom, Phil",
		booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
		month = jul,
		year = "2017",
		address = "Vancouver, Canada",
		publisher = "Association for Computational Linguistics",
		url = "https://aclanthology.org/P17-1015",
		doi = "10.18653/v1/P17-1015",
		pages = "158--167",
}

@inproceedings{hendrycksmath2021,
		title={Measuring Mathematical Problem Solving With the MATH Dataset},
		author={Dan Hendrycks and Collin Burns and Saurav Kadavath and Akul Arora and Steven Basart and Eric Tang and Dawn Song and Jacob Steinhardt},
		journal={NeurIPS},
		year={2021}
}

@inproceedings{Liu2020LogiQAAC,
		title={LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning},
		author={Jian Liu and Leyang Cui and Hanmeng Liu and Dandan Huang and Yile Wang and Yue Zhang},
		booktitle={International Joint Conference on Artificial Intelligence},
		year={2020}
}

@inproceedings{zhong2019jec,
		title={JEC-QA: A Legal-Domain Question Answering Dataset},
		author={Zhong, Haoxi and Xiao, Chaojun and Tu, Cunchao and Zhang, Tianyang and Liu, Zhiyuan and Sun, Maosong},
		booktitle={Proceedings of AAAI},
		year={2020},
}

@article{Wang2021FromLT,
		title={From LSAT: The Progress and Challenges of Complex Reasoning},
		author={Siyuan Wang and Zhongkun Liu and Wanjun Zhong and Ming Zhou and Zhongyu Wei and Zhumin Chen and Nan Duan},
		journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
		year={2021},
		volume={30},
		pages={2201-2216}
}
			 </code></pre>
		</div>

		<br>
		<hr>
	</div>
	</div>


	<script>
		$(document).ready(function() {
			// Accordion Logic
			$('.accordion-header').on('click', function() {
				const $thisHeader = $(this);
				const $thisContent = $thisHeader.next('.accordion-content');
		
				// If this header is already active, do nothing (or optionally collapse it)
				// To collapse if clicked again, uncomment the next lines:
				// if ($thisHeader.hasClass('active')) {
				//     $thisHeader.removeClass('active');
				//     $thisContent.removeClass('active').slideUp(300); // Use slideUp for animation
				//     return;
				// }
		
				// If we want only one section open at a time:
				const $container = $thisHeader.closest('.accordion-container');
				const $activeHeader = $container.find('.accordion-header.active');
				const $activeContent = $container.find('.accordion-content.active');
		
				// Deactivate and hide the currently active section (if it's not the one clicked)
				if ($activeHeader.length > 0 && !$activeHeader.is($thisHeader)) {
					$activeHeader.removeClass('active');
					// $activeContent.removeClass('active').slideUp(300); // Use slideUp for animation
					$activeContent.removeClass('active'); // Use this if not using slideUp/Down
				}
		
				// Activate and show the clicked section
				$thisHeader.addClass('active');
				$thisContent.addClass('active').show();    // ‚Üê ensures it‚Äôs visible

			});

			const deepLink = window.location.hash;



			function openAccordion(headerId) {
				const $hdr = $('#' + headerId);
				if ($hdr.length && !$hdr.hasClass('active')) $hdr.trigger('click');
				}

				// respond to in-page TOC clicks
				$('.toc-link').on('click', function (e) {
				e.preventDefault();                          // keep control of timing
				const targetId = this.getAttribute('href').substring(1);

				openAccordion(targetId);                     // expand the right panel

				// scroll only after layout has updated
				setTimeout(() => {
					document.getElementById(targetId)
							.scrollIntoView({behavior:'smooth', block:'start'});
					history.pushState(null, '', '#' + targetId);   // update URL bar
				}, 50);                                        // 50 ms is plenty
				});


				// always show the Short Version panel on first load
				openAccordion('short');

				// honour deep links (only when there *is* a hash in the URL)
				if (deepLink) {
				openAccordion(deepLink.substring(1));                 // expand the right panel
				setTimeout(() => {
					document
					.querySelector(deepLink)
					.scrollIntoView({behavior: 'smooth', block: 'start'});
				}, 0);
			}



			// --- initial open panel, honouring any #hash in the URL ---			
			openAccordion(initialHash.substring(1));   // open + show


		
			// --- Keep your existing dark mode toggle and email script ---
			const darkModeToggle = document.getElementById('darkModeToggle');
			const toggleLabel = document.getElementById('toggleLabel');
			const currentTheme = localStorage.getItem('theme');
		
			if (currentTheme === 'dark') {
				document.body.classList.add('dark-mode');
				darkModeToggle.checked = true;
				toggleLabel.textContent = 'Dark';
			} else {
				toggleLabel.textContent = 'Light';
			}
		
			darkModeToggle.addEventListener('change', function() {
				if (this.checked) {
					document.body.classList.add('dark-mode');
					localStorage.setItem('theme', 'dark');
					toggleLabel.textContent = 'Dark';
				} else {
					document.body.classList.remove('dark-mode');
					localStorage.setItem('theme', 'light');
					toggleLabel.textContent = 'Light';
				}
			});
		
			// Email obfuscation
			var eml = "sam.paech";
			var dom = "gmail.com";
			document.getElementById("email").innerHTML = '<a href="mailto:' + eml + '@' + dom + '">' + 'Email' + '</a>';
		
			// Expando for citations
			$('#expando-btn').on('click', function() {
				$('.expando-content').slideToggle();
			});
			$('.expando-content').hide(); // Initially hide citations
		
		});
	</script>



	<!-- jQuery and Bootstrap JS -->
	<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"></script>
	<script src="https://cdn.datatables.net/1.10.19/js/jquery.dataTables.min.js"></script>
	<script src="https://cdn.datatables.net/1.10.19/js/dataTables.bootstrap4.min.js"></script>
	<script src="eqbench3.js"></script>

	<!-- Cloudflare Web Analytics --><script defer src='https://static.cloudflareinsights.com/beacon.min.js' data-cf-beacon='{"token": "1600fe1d3da743c1b311f4e5fee86767"}'></script><!-- End Cloudflare Web Analytics -->
</body>
</html>