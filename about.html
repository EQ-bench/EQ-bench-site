<!DOCTYPE html>
<html lang="en">
<head>	
	<meta charset="UTF-8">
	<title>EQ-Bench Leaderboard</title>
	<!-- Bootstrap CSS -->
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<!-- DataTables Bootstrap CSS -->		
	<link rel="stylesheet" type="text/css" href="https://cdn.datatables.net/1.10.19/css/dataTables.bootstrap4.min.css">
	<link rel="stylesheet" type="text/css" href="style.css">
	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/simplelightbox/2.10.3/simple-lightbox.min.css" />
	<script src="https://cdnjs.cloudflare.com/ajax/libs/simplelightbox/2.10.3/simple-lightbox.min.js"></script>
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="pragma" content="no-cache">
</head>
<body>
	<div class="container mt-4">
		<!-- Dark/Light Mode Toggle -->
		<div class="form-check form-switch">
			<input class="form-check-input" type="checkbox" id="darkModeToggle">
			<label class="form-check-label" for="darkModeToggle" id="toggleLabel">Light</label>
		</div>
		<div class="header">
			<a href="./"><img src="./images/eqbench_logo_sml.png" alt="EQ-bench Logo" class="logo"/></a>
			<div class="header-text">
				<h1>EQ-Bench</h1>				 
			</div>
		</div>
	  <p>Emotional Intelligence Benchmark for LLMs</p>
		
		<p><a href="https://github.com/EQ-bench" target="_blank">Github</a> | <a href="https://arxiv.org/abs/2312.06281" target="_blank">Paper</a> | <span id="email"></span> | <a href="https://twitter.com/sam_paech" target="_blank">Twitter</a> | <a href="index.html">Leaderboard</a></p>

		<p>
            <a href="index.html">üíôEQ-Bench3</a> |
            <a href='creative_writing.html'>üé®Creative Writing</a> |
            <a href='judgemark-v2.html'>‚öñÔ∏èJudgemark v2</a> |
            <a href='buzzbench.html'>üé§BuzzBench</a> |
            <a href='eqbench-v2.html'>üíóEQ-Bench (Legacy)</a>
        </p>

		<p><script type='text/javascript' src='https://storage.ko-fi.com/cdn/widget/Widget_2.js'></script><script type='text/javascript'>kofiwidget2.init('Support Me on Ko-fi', '#1a1a1a', 'O5O7VUVYO');kofiwidget2.draw();</script> </p>

		<div class="toc">
			<ul>
				 <li><a href="#about">How to Submit</a></li>
				 <li><a href="#eq-bench">EQ-Bench</a></li>
				 <li><a href="#magi-hard">MAGI-Hard</a></li>
				 <li><a href="#creative-writing">Creative Writing</a></li>
				 <li><a href="#judgemark">Judgemark</a></li>
				 <li><a href="#buzzbench">BuzzBench</a></li>
				 <li><a href="#citations">Citations</a></li>
			</ul>
	  </div>
		
		<div id="about" class="section">
			<h3>üì©How to Submit</h3>
			<p>At this time we only accept submissions of open weight models that are available to everyone via HuggingFace.</p>
			<p>To submit, get in touch by email or twitter with:
				<ul>
					<li>A link to your model on huggingface</li>
					<li>Optimal prompt format & generation config</li>
					<li>The EQ-Bench score that you got for your model</li>
				</ul>
			</p>
			<p>We will then verify the result on our end and add to the leaderboard. This project is self funded so please respect that we don't have unlimited compute!</p>
		</div>
		<div id="eq-bench3" class="section">
			<h3>üíôEQ-Bench 3</h3>
			<p>EQ-Bench 3 is an Emotional Intelligence benchmark focused on active conflict mediation skills. It evaluates the ability of language models to navigate complex emotional territory in challenging and diverse scenarios.</p>
			
			<p>The benchmark employs a set of conflict scenarios where the model must mediate between two or more parties with conflicting interests, emotional states, and communication patterns. Unlike the original EQ-Bench which assesses the ability to predict emotional states, EQ-Bench 3 targets active emotional intelligence abilities.</p>
			
			<p><strong>Methodology:</strong></p>
			<ul>
				<li><strong>Test Structure:</strong> The benchmark runs multi-turn conversations (up to 21 turns) between the test model (acting as conflict mediator) and actor models (playing clients or disputants). The actor model we use is gemini-2.0-flash-001. Each scenario includes detailed character profiles with specific emotional states and backgrounds.</li>
				
				<li><strong>Assessment Criteria:</strong> We score models on:
					<ul>
						<li>Basic emotional intelligence skills (recognizing emotions, showing empathy)</li>
						<li>Professional skills specific to therapy or mediation</li>
						<li>Avoiding serious professional mistakes</li>
					</ul>
				</li>
				
				<li><strong>How It Works:</strong> The benchmark uses three models:
					<ul>
						<li>Test model: The AI being evaluated</li>
						<li>Actor model: Plays realistic clients or disputants</li>
						<li>Judge model: Claude-3.7-Sonnet scores the test model's performance</li>
					</ul>
				</li>
				
				<li><strong>Scoring:</strong> The final score combines:
					<ul>
						<li>Scores across multiple skill areas</li>
						<li>A count of identified mis-steps and how serious they were</li>
					</ul>
				</li>
			</ul>
			
			<p>Beyond just scores, the judge provides a critical analysis of specific errors, rating them as minor, moderate, or serious. This helps identify exactly where and how models struggle in realistic professional conversations.</p>

			<p><b>Judge Self-Bias</b></p>
			<p>A common concern raised about LLM-judge evals is that the judge might be biased, particularly towards its own outputs. To understand this effect we benchmarked the top models with two judges: Sonnet-3.7 (the judge we use for the leaderboard), and gpt-4o-2024-11-20.</p>
			<p>Here are the results from these two judge models rating the top models (including themselves) in the EQ-Bench3 task:</p>
			<a href="./images/eqbench3-judge-comparison.png" class="lightbox">
				<img src="./images/eqbench3-judge-comparison.png" style="width: 900px; max-width: 100%; height: auto;" />
			  </a>
			<p>While this is a limited comparison, we don't see any significant self-bias to be evident for these two judges. In fact, their scoring is nearly identical.</p>
			
			<p>Code and full documentation will be available in the EQ-Bench repository soon.</p>
			<br>
		</div>
		
		
		
		<div id="eq-bench" class="section">
			<h3>üíóEQ-Bench</h3>
			<p>EQ-Bench is a benchmark for language models designed to assess emotional intelligence.</p>
			<p>Why emotional intelligence? One reason is that it represents a subset of abilities that are important for the user experience, and which isn't explicitly tested by other benchmarks. Another reason is that it's not trivial to improve scores by fine tuning for the benchmark, which makes it harder to "game" the leaderboard.</p>
			<p>EQ-Bench is a little different from traditional psychometric tests. It uses a specific question format, in which the subject has to read a dialogue then rate the intensity of possible emotional responses of one of the characters. Every question is interpretative and assesses the ability to predict the magnitude of the 4 presented emotions. The test is graded without the need for a judge (so there is no length bias). It's cheap to run (only 171 questions), and produces results that correlate strongly with human preference (Arena ELO) and multi-domain benchmarks like MMLU.</p>
			<p>You can run the benchmark on your own models or validate the leaderboard scores using the code in the github repo above.</p>
			<p>If you would like to see a model on the leaderboard, get in touch and suggest it!</p>
			<br>
		</div>
		<div id="magi-hard" class="section">
			<h3>üßôMAGI-Hard</h3>
			<p>LLM Benchmarks are chasing a moving target and fast running out of headroom. They are struggling to effectively separate SOTA models from leaderboard optimisers. Can we salvage these old dinosaurs for scrap and make a better benchmark?</p>
			<p>MAGI-Hard is a recently added metric to the leaderboard. It is a custom subset of MMLU and AGIEval, selected to have strong discriminatory power between top ability models.</p>
			<p>Read more <a href="https://sampaech.substack.com/p/creating-magi-a-hard-subset-of-mmlu" target="_blank">here</a>.</p>
			<p>You can use the MAGI test sets with <a href="https://github.com/sqrkl/lm-evaluation-harness" target="_blank">this fork of EleutherAI lm-evaluation-harness</a>.</p>
			<br>
		</div>
		<div id="creative-writing" class="section">
			<h3>üé®Creative Writing</h3>
			<p>This benchmark uses a LLM judge (Claude 3.5 Sonnet) to assess the creative writing abilities of the test models on a series of writing prompts.</p>
			<p>You can reproduce these results or run the benchmark on your own models with the <a href="https://github.com/EQ-bench" target="_blank">EQ-Bench repo on Github</a>.</p>
			<p><b>Update 2025-02-25: New metric -- Vocab Complexity</b></p>
			<p>It's become apparent that the judge in this eval is easily impressed by vocab flexing. Some of the models tested use an inordinate amount of complex multisyllabic vocabulary, and it artificially inflates their score. As such we've introduced a new column for vocab complexity ("Vocab"), using a calculation of the proportion of words having 3+ syllables.</p>
			<p>The "Vocab control" slider penalises overly complex vocab usage. It may seem counter-intuitive to penalise complex vocab, but in our experience, vocab-maxxing harms writing quality. Since this is quite a subjective aspect to the evaluation, we let the user set the penalty amount.</p>
			<p><b>GPT-Slop</b></p>
			<p>The "Slop" metric measures words that are typically over-used by LLMs (also known as GPT-isms). Higher values == more slop. It calculates a value representing how many words in the test model's output match words that are over-represented in typical language model writing. We compute the list of "gpt slop" words by counting the frequency of words in a large dataset of generated stories (<a href="https://huggingface.co/datasets/ajibawa-2023/General-Stories-Collection">Link to dataset</a>).</p>
			<p>Some additional phrases have been added to the slop list as compiled from similar lists around the internet.</p>
			<p>The full list, as well as the code to generate the over-represented words, can be found here: <a href="https://github.com/sam-paech/antislop-sampler">https://github.com/sam-paech/antislop-sampler</a>.</p>
			<p>If you're interested in reducing gpt-isms, you can try the anti-slop sampler found in this repo. It downregulates the probability of the provided phrase list as the model inferences.</p>
			<hr />
			<p>We've released v2 of the creative writing benchmark & leaderboard. The old version was starting to saturate (scores bunching at the top), so we removed some of the less discriminative prompts, switched judge models, and made some other improvements besides.</p>
			<p><b>Version 2 Changes</b></p>
			<ul>
				<li>Default min_p = 0.1, temp = 1 for transformers & oobabooga inference</li>
				<li>Change to Claude 3.5 Sonnet as judge</li>
				<li>Removed some prompts and added new ones; 24 in total now.</li>
				<li>Reworked the scoring criteria</li>
				<li>Criteria now are weighted (to increase discriminative power)</li>
				<li>Leaderboard models are now tested for 10 iterations</li>
				<li>Leaderboard now shows error bars for 95% confidence interval</li>
				<li>Sample txt on leaderboard now show scores for all iterations, as well as inference settings</li>
			</ul>
			<p>There has been a distinct lack of automated benchmarks for creative writing because, put simply, it's hard to assess writing quality without humans in the loop. Asking a language model, "How good is this writing (0-10)" elicits poor results. Even if we had a good LLM judge, it's not immediately obvious how to formalise the assessment of creative writing objectively.</p>
			<p>The release of Claude 3, in particular the flagship Opus model, has solved half of this equation: it's able to give meaningful & nuanced analysis of creative writing output, and it can tell the difference between a wide range of ability levels.</p>
			<p>To solve the other half of the equation, we've come up with an assessment format that works to the strengths of LLM judges and avoids their weaknesses. LLM judges are typically bad at scoring nebulous metrics like "How well written is this piece?" They also find it hard to give consistent scores on an objective rating system if they don't have some exemplar or baseline reference to compare to.</p>
			<p>Our test includes:</p>
			<ul>
				<li>24 writing prompts assessed over 10 iterations</li>
				<li>27 narrowly defined assessment criteria</li>
				<li>Including 6 question-specific criteria</li>
				<li>Several criteria targeting positivity bias which (in our opinion) contributes to bad writing</li>
				<li>Exemplar reference output for each question</li>
			</ul>
			<p>This approach of breaking down the assessment task into a granular set of criteria and comparing to an exemplar has brought creative writing assessment into the purview of LLM judges. Our test is discriminative amongst a wide range of writing ability levels.</p>
			<p><b>* A note on biases *</b></p>
			<p>LLM judges have biases. LLM-as-a-judge benchmarks such as Alpaca-Eval can exhibit a strong length bias where the judge, (in Alpaca-Eval's case GPT-4), prefers longer outputs. Their approach involves presenting the output from two models to the judge, and the judge says which it thinks is better.</p>
			<p>We attempt to mitigate the length bias by: A. assessing by 27 narrow criteria, and B. explicitly instructing the judge not to be biased by length (this seems to work for MT-Bench).</p>
			<p>As of version 2, we now include length control slider which scales the score up or down depending on whether the average output length for a given model is above or below the average for all models. This is an attempt to control the bias where the judge model tends to favour longer outputs. With the slider at 0%, no length scaling is applied. With the slider at 100%, the scores are scaled by up to 10%. This length control implementation is somewhat arbitrary; it's not really possible to precisely control for this bias, as we can't meaningfully hold the writing quality equal while varying the length. It does seem likely/evident that some degree of length bias is present, and has set the default LC parameters according to our rough intuitive guess (science!).</p>
			<p>It's possible / likely that this & other biases might still be a factor in scoring (e.g. Claude might prefer its own and other anthropic models). So bear this in mind when interpreting the results.</p>
			<p>We include the outputs that the model generated for each prompt so you can judge for yourself.</p>
			<p><b>Alternative Judge Models</b></p>
			<p>Yes, you can use other judge models than Claude Opus (although the results won't be directly comparable). Currently the benchmark pipeline supports Anthropic, OpenAI and Mistral models via their APIs. Soon we will support local models as judges.</p>
			<p><b>* A note on variance *</b></p>
			<p>This benchmark has a relatively small number of test questions (19). We specify generation temperature = 0.7 so each run is different. This means there is significant variation of scores between iterations (avg range: 3.35, std dev: 1.41). To reduce variance we recommend using 3 iterations or more. The leaderboard scores are averaged over 3 iterations.</p>
			<p>It costs around $3.00 to bench a model over 3 iterations using Claude 3 Opus at current rates.</p>
			<p>If you would like your model included on the creative writing leaderboard, please consider contributing to my compute costs, and get in touch!</p>
		</div>

		<div id="judgemark" class="section">
			<h3>‚öñÔ∏è Judgemark V2</h3>
			<p>
			  <strong>Judgemark V2</strong> is a major update to our original ‚Äújudge‚Äù benchmark for creative-writing evaluation. 
			  The benchmark measures how well a language model can <em>numerically grade</em> a diverse set of short fiction outputs, 
			  using a detailed rubric of positive and negative criteria. It goes beyond simpler pairwise preference tests by 
			  requiring the judge to follow complex instructions, parse each story, and produce scores for up to 36 different 
			  literary qualities. 
			</p>
		  
			<p>
			  <strong>Key improvements over V1 include:</strong>
			  <ul>
				<li><em>6x more samples per writer model</em>, reducing variance between benchmark runs.</li>
				<li><em>Refined scoring metrics</em> that capture ‚Äúseparability‚Äù (whether the judge can distinguish strong vs. weak writing) and ‚Äústability‚Äù (how consistent its rankings are across multiple runs), as well as correlation to human preference.</li>
				<li><em>Calibrated & raw scores</em>: We show two final Judgemark scores. ‚ÄúRaw‚Äù is how the judge performs out-of-the-box, while ‚ÄúCalibrated‚Äù normalizes the judge‚Äôs score distribution so that it can be compared more fairly to other judges.</li>
				<li><em>Perturbation stability</em>: We run the judge at temp=0.5, top_k=3. This is intended to introduce variation between runs, so that we can assess the stability of scores & rankings to perturbation. This is a crucial part of understanding whether we're measuring the thing we intend to measure, and not just random fluctuations.</li>
				<li><em>Simplified codebase</em>: A new codebase was created for v2, separate from the EQ-Bench code. It's simpler and handles concurrent threads.</li>
			  </ul>
			</p>

			<p>
				<strong>Repeatability Results</strong>
				<p>We tested Llama-3.1-70B-instruct 20 times to test the repeatability of the final Judgemark score (tests were run at temp=0.5, top_k=3).</p>
				<pre class="code-block">
					<code>
llama-3.3-70b_judgemark_scores = [
	55.7, 54.4, 55.4, 56.7, 55.0, 56.3, 57.0, 54.5, 55.6, 56.1,
	54.9, 57.5, 55.0, 53.8, 54.7, 56.2, 55.7, 54.6, 55.4, 56.6, 54.0
]

Mean Score: 55.481
Standard Deviation: 1.004
Range (Max - Min): 3.67
Coefficient of Variation: 0.0181
					</code>
				</pre>
			<p>
			  <strong>The Judging Task</strong>:  
			  Each test item is a short creative piece generated by one of 17 ‚Äúwriter models‚Äù. These models' writing abilities are an even spread from weak to strong. 
			  The judge model receives a lengthy prompt that includes (a) the writing prompt itself, 
			  (b) the test model‚Äôs story, and (c) an extensive list of scoring instructions (for example, 
			  ‚ÄúNuanced Characters: 0‚Äì10,‚Äù ‚ÄúOverwrought: 0‚Äì10‚Äù, etc.). 
			  The judge must then output numeric scores for each criterion. We parse those scores and aggregate them 
			  into a single <em>aggregated_score_raw</em> for each piece. Some criteria like ‚ÄúWeak Dialogue‚Äù are marked <em>lower is better</em> in the judging prompt which adds additional complexity to the task.
			</p>
		  
			<p>
			  <strong>Final Judgemark Score</strong>:  
			  After scoring all stories from multiple writers, we track how the judge‚Äôs ratings compare to known references 
			  and how well they separate the better texts from weaker ones. We also measure how consistently the judge‚Äôs 
			  rankings repeat if we prompt it again, and compute correlation with human preference rankings (per <a href="https://lmarena.ai/" target="_blank">Chatbot Arena</a> ELO scores).
			  The final Judgemark formula is a weighted sum of these computed metrics. See the formula at the bottom of the leaderboard page <a href="/judgemark-v2.html">here.</a>
			</p>
		  
			<p>
			  <strong>Interpreting the Leaderboard</strong>:  
			  In the table, ‚ÄúScore (Calibrated)‚Äù is typically higher if a judge effectively uses the full range of scores 
			  (once normalized), strongly differentiates strong vs. weak writing, and correlates with human preferences. 
			  ‚ÄúScore (Raw)‚Äù shows how the judge performed before any normalization. ‚ÄúStability‚Äù indicates how consistent the 
			  judge‚Äôs assigned rankings remain across repeated trials. ‚ÄúSeparability‚Äù highlights the judge‚Äôs ability to 
			  keep higher- and lower-quality outputs well apart. 
			</p>
		  
			<p>
			  This is a particularly <em>difficult</em> task for LLMs, as it involves nuanced literary critique and 
			  instructs models to use a multi-dimensional numeric scale‚Äîan area where many generative models still struggle.
			</p>
		  
			<p>
			  Source code for running Judgemark v2 can be found here: <a href="https://github.com/EQ-bench/Judgemark-v2">https://github.com/EQ-bench/Judgemark-v2</a>.			  
			</p>
		</div>
		  

		<div id="buzzbench" class="section">
			<h3>üé§BuzzBench</h3>
			<p>A humour analysis benchmark.</p>
			<p><a href="https://huggingface.co/datasets/sam-paech/BuzzBench-v0.60">BuzzBench dataset on Huggingface</a></p>
			<p>Do you enjoy seeing the jokes from your favourite shows dissected with a blunt machete? Well, you found the right benchmark.</p>

			<p>The task of explaining traditionally constructed jokes is actually pretty straightforward for modern LLMs. So we made things more difficult:</p>
			<ul>
				<li>We use the guest intros in the British music pop quiz show <a href="https://www.bbc.co.uk/programmes/b006v0dz" target="_blank">Never Mind The Buzzcocks</a>, because the intro jokes are variously subtle, risque, lazy, obscure, obvious and clever. LLMs find these distinctions hard.</li>
				<li>In addition to explaining how the joke works, the LLM has to predict whether the joke is actually funny (to the audience and to a comedy writer).</li>
				<li>The responses are scored by a LLM judge against a human-authored gold response.</li>
			</ul>

			<p>The responses from SOTA models typically miss a lot of the humour, predict the funniness badly, fabricate and over-analyse. That's good! It's meant to be a hard test. The task encodes some deep complexities including theory of mind understanding and requires an intricate understanding of how jokes work. The show is also very British and exists in a dated cultural context, increasing the interpretation challenge.</p>
			<p>"Humour is so subjective -- so how can you even make a benchmark around that?"</p>
			<p>This benchmark is as much about predicting human responses to jokes as it is about joke deconstruction. The questions are explicitly framed around analysing the jokes from the perspective of the show's audience, and from the perspective of a comedy writer. The human authored gold answers ground the judge's answers in a real human's sense of humour. This shifts the task from being about subjective taste to being about modeling human response to jokes.</p>
			<p>The intention for the task design is for there to be significant (nontrivial) headroom on the benchmark as language models get better at getting inside our heads.</p>
			<p><b>The Judge: Claude 3.5 Sonnet.</b> We picked Sonnet 3.5 to act as the judge partly because it scores highest on the Judgemark leaderboard, and partly because it seems least biased to favour longwinded, over-analysed, over-reaching responses. Which is a common failure mode in respondent answers, and something other judges are more easily led astray by.</p>
			<p>* A note on judge self-bias:</p>
			<p>We can expect there could be some degree of self-bias with the judge preferring its own outputs, although this is difficult to quantify and disentangle from other sources of bias. We should remain aware that LLM judge benchmarks are not perfect. The upside of a LLM judge using a scoring rubric is that we get nice interpretable results in the form of the judge's analysis and scores. So we have good visibility on whether the judge is doing its job, and can decide for ourselves whether the respondent models are indeed getting the humour, or just talking shite.</p>
			<p>Models are evaluated using openrouter with temp=0.7. Several (typically 5-10) iterations are performed per model to establish confidence intervals and mitigate variance.</p>
			<p>BuzzBench source code will be released soon.</p>
			<p><em>Never Mind The Buzzcocks is a TV series developed by the BBC. Our usage of the work in BuzzBench is non-commercial educational & research, using only a small excerpt of the show's transcript which falls under fair use or "fair dealing" in UK copyright law.</em></p>
		</div>

		<p><script type='text/javascript' src='https://storage.ko-fi.com/cdn/widget/Widget_2.js'></script><script type='text/javascript'>kofiwidget2.init('Support Me on Ko-fi', '#1a1a1a', 'O5O7VUVYO');kofiwidget2.draw();</script> </p>

		<hr>

		<div id="citations" class="section">
		<h5>Cite EQ-Bench:</h5>
		<pre><code>
@misc{paech2023eqbench,
	title={EQ-Bench: An Emotional Intelligence Benchmark for Large Language Models}, 
	author={Samuel J. Paech},
	year={2023},
	eprint={2312.06281},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}
		</code></pre>	
		MAGI draws from the MMLU and AGIEval tests. <span class="clickable-text" id="expando-btn" style="cursor: pointer; text-decoration: underline; color: blue;">Click to show citations</span>
		<div class="expando-content mt-3" style="overflow-x: auto;">
			 <pre><code>
@article{hendryckstest2021,
		title={Measuring Massive Multitask Language Understanding},
		author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
		journal={Proceedings of the International Conference on Learning Representations (ICLR)},
		year={2021}
}

@article{hendrycks2021ethics,
		title={Aligning AI With Shared Human Values},
		author={Dan Hendrycks and Collin Burns and Steven Basart and Andrew Critch and Jerry Li and Dawn Song and Jacob Steinhardt},
		journal={Proceedings of the International Conference on Learning Representations (ICLR)},
		year={2021}
}

@misc{zhong2023agieval,
		title={AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models},
		author={Wanjun Zhong and Ruixiang Cui and Yiduo Guo and Yaobo Liang and Shuai Lu and Yanlin Wang and Amin Saied and Weizhu Chen and Nan Duan},
		year={2023},
		eprint={2304.06364},
		archivePrefix={arXiv},
		primaryClass={cs.CL}
}

@inproceedings{ling-etal-2017-program,
		title = "Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems",
		author = "Ling, Wang  and
		Yogatama, Dani  and
		Dyer, Chris  and
		Blunsom, Phil",
		booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
		month = jul,
		year = "2017",
		address = "Vancouver, Canada",
		publisher = "Association for Computational Linguistics",
		url = "https://aclanthology.org/P17-1015",
		doi = "10.18653/v1/P17-1015",
		pages = "158--167",
}

@inproceedings{hendrycksmath2021,
		title={Measuring Mathematical Problem Solving With the MATH Dataset},
		author={Dan Hendrycks and Collin Burns and Saurav Kadavath and Akul Arora and Steven Basart and Eric Tang and Dawn Song and Jacob Steinhardt},
		journal={NeurIPS},
		year={2021}
}

@inproceedings{Liu2020LogiQAAC,
		title={LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning},
		author={Jian Liu and Leyang Cui and Hanmeng Liu and Dandan Huang and Yile Wang and Yue Zhang},
		booktitle={International Joint Conference on Artificial Intelligence},
		year={2020}
}

@inproceedings{zhong2019jec,
		title={JEC-QA: A Legal-Domain Question Answering Dataset},
		author={Zhong, Haoxi and Xiao, Chaojun and Tu, Cunchao and Zhang, Tianyang and Liu, Zhiyuan and Sun, Maosong},
		booktitle={Proceedings of AAAI},
		year={2020},
}

@article{Wang2021FromLT,
		title={From LSAT: The Progress and Challenges of Complex Reasoning},
		author={Siyuan Wang and Zhongkun Liu and Wanjun Zhong and Ming Zhou and Zhongyu Wei and Zhumin Chen and Nan Duan},
		journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
		year={2021},
		volume={30},
		pages={2201-2216}
}
			 </code></pre>
		</div>

		<br>
		<hr>
	</div>
	</div>

	<!-- jQuery and Bootstrap JS -->
	<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"></script>
	<script src="https://cdn.datatables.net/1.10.19/js/jquery.dataTables.min.js"></script>
	<script src="https://cdn.datatables.net/1.10.19/js/dataTables.bootstrap4.min.js"></script>
	<script src="eqbench3.js"></script>
	<script>
		document.addEventListener('DOMContentLoaded', function() {
		  new SimpleLightbox('.lightbox');
		});
	</script>
	<!-- Cloudflare Web Analytics --><script defer src='https://static.cloudflareinsights.com/beacon.min.js' data-cf-beacon='{"token": "1600fe1d3da743c1b311f4e5fee86767"}'></script><!-- End Cloudflare Web Analytics -->
</body>
</html>