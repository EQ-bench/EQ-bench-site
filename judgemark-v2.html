<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>EQ-Bench Judgemark v2.1 Leaderboard</title>
  <!-- Bootstrap CSS -->
  <link
    href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css"
    rel="stylesheet">
  <!-- DataTables Bootstrap CSS -->
  <link
    rel="stylesheet"
    type="text/css"
    href="https://cdn.datatables.net/1.10.19/css/dataTables.bootstrap4.min.css">
  <link rel="stylesheet" type="text/css" href="style.css">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate">
  <meta http-equiv="Pragma" content="no-cache">
  <meta http-equiv="Expires" content="0">
  <style>
    
  </style>
</head>
<body>
  <div class="container mt-4">
    <!-- Dark/Light Mode Toggle -->
    <div class="form-check form-switch">
      <input class="form-check-input" type="checkbox" id="darkModeToggle">
      <label class="form-check-label" for="darkModeToggle" id="toggleLabel">
        Light
      </label>
    </div>

    <div class="header">
      <a href="./"><img src="./images/eqbench_logo_sml.png" alt="EQ-bench Logo" class="logo"/></a>
      <div class="header-text">
        <h1>Judgemark v2.1</h1>
      </div>
    </div>

    <p>Emotional Intelligence Benchmarks for LLMs</p>
    <p>
      <a href="https://github.com/EQ-bench" target="_blank">Github</a> |
      <a href="https://arxiv.org/abs/2312.06281" target="_blank">Paper</a> |
      <span id="email"></span> |
      <a href="https://twitter.com/sam_paech" target="_blank">Twitter</a> |
      <a href="about.html">About</a>
    </p>

    <p>
        <a href='index.html'>ğŸ’™EQ-Bench3</a>
        <span class="nav-separator">|</span>
        <a href='spiral-bench.html'>ğŸŒ€Spiral-Bench v1.2</a>
        <span class="nav-separator">|</span>
        <a href='creative_writing_longform.html'>âœï¸Longform Writing</a>
        <span class="nav-separator">|</span>
        <a href='creative_writing.html'>ğŸ¨Creative Writing v3</a>
        <span class="nav-separator">|</span>
        <b>âš–ï¸Judgemark v2.1</b>
        <span class="nav-separator">|</span>
        <a href='buzzbench.html'>ğŸ¤BuzzBench</a>
        <span class="nav-separator">|</span>
        <a href='diplobench.html'>ğŸŒDiploBench</a>
        <span class="nav-separator">|</span>
        <span class="nav-dropdown">
          <button class="nav-dropdown-toggle" id="legacyDropdownToggle">
            ğŸ“šLegacy Leaderboards
          </button>
          <span class="nav-dropdown-menu" id="legacyDropdownMenu">
            <a href="spiral-bench_v1.0.html" class="nav-dropdown-item">ğŸŒ€Spiral-Bench v1.0</a>
            <a href="creative_writing_v2.html" class="nav-dropdown-item">ğŸ¨Creative Writing v2</a>
            <a href="eqbench-v2.html" class="nav-dropdown-item">ğŸ’—EQ-Bench v2</a>
          </span>
        </span>
    </p>

  

    <p>
      A benchmark measuring LLM judging ability.
      <a href="./about.html#judgemark">Learn more</a>
    </p>
    <p>View <a href="./model_pareto_plot_dark.html" target = "_blank">Pareto Plot</a>ğŸ“ˆ</p>

    <!-- Toggle for mobile detail expansion -->
    <button id="toggleMiddleStats" class="btn btn-primary d-block d-lg-none mb-2">
      Expand Details
    </button>

    <div class="table-responsive">
      <table
        id="judgemark-leaderboard-v2"
        class="table table-striped table-bordered">
        <thead>
          <tr>
            <th>Model</th>
            <th class="calibrated-score-col">Judgemark Score</th>
            <th>Stability</th>
            <th>Separability</th>
            <th>Human Corr</th>
            <th>Cost</th>            
            <th></th>  <!-- Charts -->
          </tr>
        </thead>
        <tbody id="leaderboardBodyV2">
          <!-- Data rows are generated by judgemark_v2.js -->
        </tbody>
      </table>
    </div>

    <br>
    <hr>

    <div class="section">
      <p><b>âš–ï¸ Judgemark v2.1</b> explanation of displayed stats:</p>
      <ul>
        <li><b><code>Judgemark Score</code></b>: Overall Judgemark score</li>
        <li><b><code>Stability</code></b>: A measure of how much the judge's assigned rankings vary between iterations</li>
        <li><b><code>Separability</code></b>: How well the judge can separate models by ability (measured by CI99 overlap with adjacent models)</li>
        <li><b><code>Human Corr</code></b>: Correlation with human preferences (per the LMSys Arena creative writing category)</li>
        <li><b><code>Cost</code></b>: The cost to complete one run of the benchmark (via openrouter)</li>
      </ul>
      <p>
        For each row, you can view:
      </p>
      <ul>
        <li><b>Stats</b> â€“ A link to the .json file with full computed statistics</li>
        <li><b>Scoring Chart</b> â€“ Plots of the judge's assigned scores to the test models (raw & calibrated), and a heatmap of the frequency each score (0-10) was given by the judge.</li>
        <li><b>Scatter Chart</b> â€“ Scatter plots of item length vs score.</li>
      </ul>
      <p>
        <b>Judgemark Score</b>		
      </p>
	  <p>
		The final Judgemark score is computed as a weighted sum of several metrics that quantify how well the judge has <b>separated</b> each model by ability, how <b>stable</b> the judge's rankings are, and how well they agree with <b>human preference</b>.
		<pre class="code-block">
			<code>
# All elements here are pre-normalised 0-1 (larger=better)		  
# Compute an aggregated separability metric
separability_agg = (
	+ kw_stat            # kruskal-wallis cluster analysis (separability)
	+ ci99_overlap       # confidence interval overlap between adjacently ranked models (separability)
) / 2.0

# Combine into final Judgemark score  
judgemark_score = (
	kendall_tau_iters    # correlation between iterations (ranking stability)
	+ kendall_tau_lmsys  # correlation with lmsys arena score (corr to human pref)        
	+ 4 * separability_agg # aggregate of separability metrics
) / 6.0</code>
		</pre>

		
	  </p>
    </div>
    <br>
  </div>

  <!-- jQuery and Bootstrap JS -->
  <script
    src="https://code.jquery.com/jquery-3.3.1.slim.min.js">
  </script>
  <script
    src="https://cdn.datatables.net/1.10.19/js/jquery.dataTables.min.js">
  </script>
  <script
    src="https://cdn.datatables.net/1.10.19/js/dataTables.bootstrap4.min.js">
  </script>

  <!-- Our Judgemark v2 script -->
  <script src="judgemark-v2.js?v=1.08"></script>

  <!-- Dropdown functionality -->
  <script src="utils.js"></script>
  <!-- Cloudflare Web Analytics --><script defer src='https://static.cloudflareinsights.com/beacon.min.js' data-cf-beacon='{"token": "1600fe1d3da743c1b311f4e5fee86767"}'></script><!-- End Cloudflare Web Analytics -->
</body>
</html>